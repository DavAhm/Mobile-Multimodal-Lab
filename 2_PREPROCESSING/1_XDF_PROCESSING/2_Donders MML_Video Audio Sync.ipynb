{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d4c572-e05b-4c3b-9db9-413b1195bf95",
   "metadata": {},
   "source": [
    "# Donders MML: Video Clipping and Audio-Video Alignment\n",
    "![alt text](<Donders MML LOGO.png>)\n",
    "\n",
    "### Info Documents \n",
    "Location Repository\n",
    "Github Repository \n",
    "Jupyter Notebook\n",
    "\n",
    "\n",
    "### Requirements\n",
    "Please install the necessary packages in requirements.txt using pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d482622",
   "metadata": {},
   "source": [
    "## 0. Importing Relevant Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a33655-b420-425b-8ece-4cac1de64c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything was imported succesfully\n"
     ]
    }
   ],
   "source": [
    "import os             # Importing the os module which provides functions for interacting with the operating system\n",
    "import pyxdf          # Importing pyxdf, a Python library for reading XDF files\n",
    "import glob           # Importing the glob module which helps in finding files/directories with specific patterns\n",
    "import pandas as pd   # Importing pandas library (abbreviated as pd), which is used for data manipulation and analysis\n",
    "import numpy as np    # Importing numpy library (abbreviated as np), which is used for numerical computations\n",
    "import wave           # Importing wave module for reading and writing WAV files (usually audio files) \n",
    "import struct         # Importing struct module which provides functions to convert between Python values and C structs\n",
    "import math           # Importing math module which provides mathematical functions\n",
    "import random         # Importing random module for generating random numbers\n",
    "from scipy.io import wavfile  # Importing wavfile module from scipy.io (a library built on numpy), for reading and writing WAV files\n",
    "import noisereduce as nr      # Importing noisereduce module for noise reduction in audio signals\n",
    "import json            # Importing json module for working with JSON data\n",
    "import cv2            # Importing OpenCV library for computer vision tasks\n",
    "from moviepy.editor import (                # Importing various classes and functions from moviepy.editor module\n",
    "                            VideoFileClip,  # Class for working with video files\n",
    "                            AudioFileClip,  # Class for working with audio files\n",
    "                            CompositeAudioClip)  # Class for composing audio clip\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip  # video  clipping fucntion \n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip          # alternative video clipping function\n",
    "import matplotlib.pyplot as plt                                   # Importing pyplot library to create figures and plot data \n",
    "from matplotlib.widgets import Slider  \n",
    "import tkinter                                                    # GUI toolkit to open and save files\n",
    "from tkinter import filedialog                                    # GUI toolkit to open and save files\n",
    "import subprocess \n",
    "from tqdm.notebook import tqdm\n",
    "import re                                                         # Importing re module for working with regular expressions\n",
    "\n",
    "\n",
    "print(\"Everything was imported succesfully\") #as terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc6018",
   "metadata": {},
   "source": [
    "## 1. Establish Relevant Paths, Variabls & Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "489fb651-acc9-43ab-bfd8-f015695f2343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input video folder = c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\video_raw\n",
      "Input file folder = c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\data_processed\\T1\\marker_MULTIPLEpairs\n",
      "Output_video folder = c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\video_clipped\\T1\\marker_MULTIPLEpairs\n",
      "Output_audiovideo folder = c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\n",
      "Function \"frame_to_time\" created sucesfully\n",
      "Function \"get_events\" created sucesfully\n"
     ]
    }
   ],
   "source": [
    "# ------------ PATHS -----------------------------------------------------\n",
    "input_video_folder  = './video_raw/'                             # this folder should only contain the videos you want to process. \n",
    "input_file_folder   = './data_processed/T1/marker_MULTIPLEpairs'    # this folder contains the csv files extracted from the XDF files\n",
    "output_video_folder = './video_clipped/T1/marker_MULTIPLEpairs'     # this folder will contain the clipped videos\n",
    "output_audiovideo_folder = './audiovideo_sync_clipped/T1/marker_MULTIPLEpairs'     # this folder will contain the audio files extracted from the videos\n",
    "\n",
    "print(\"Input video folder =\", os.path.abspath(input_video_folder))\n",
    "print(\"Input file folder =\", os.path.abspath(input_file_folder))\n",
    "print(\"Output_video folder =\", os.path.abspath(output_video_folder))\n",
    "print(\"Output_audiovideo folder =\", os.path.abspath(output_audiovideo_folder))\n",
    "\n",
    "\n",
    "# ------------ VARIABLES -----------------------------------------------------\n",
    "# FILE-VIDEO MATCHING: List of each participant csv file e.g., ((e.g., xx_xx_Video_P1.csv) and the corresponding raw video (e.g., xx_Video_P1.avi)\n",
    "participants_file_video_mapping = [\n",
    "    ('Video_P1', 'P1'),\n",
    "    ('Video_P2', 'P2')  # Add more participants file-video mappings as needed      \n",
    "]\n",
    "\n",
    "\n",
    "# AUDIO-VIDEO MATCHING: Regular expression pattern to match the participant's video file name. Change as needed. \n",
    "audio_video_pattern_matching = re.compile(r'.(P\\d+)_(Vision|NoVision)_(Movement|NoMovement)_\\d+_StartParticipantSinging.*(Vision|NoVision)_(Movement|NoMovement)_\\d+_EndParticipantSinging')\n",
    "\n",
    "\"\"\"In our case the files are named as [...]_PX_XVision_XMovement_X_StartParticipantSinging_XVision_XMovement_X_EndParticipantSinging[...]\n",
    "        where X are changing expression inside this regular pattern. \n",
    "        The .* at the beginning and end allow for any (or no) prefix and sufffix \n",
    "    The regular expression is used to match the file names between audios and videos audiio audio-video alignment\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary to map file extensions to FourCC codecs\n",
    "extension_to_codec = {\n",
    "    '.mp4': 'H264',  # 'H264' is a valid FourCC code for H.264\n",
    "    '.avi': 'XVID',  # 'XVID' is a common FourCC for AVI\n",
    "    '.mov': 'H264',  # Again, use 'H264' or 'MP4V'\n",
    "    '.mkv': 'H264',  # Use 'H264' for MKV files too\n",
    "    '.flv': 'FLV1',  # 'FLV1' is a valid codec for FLV\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "\n",
    "# ------------ FUNCTIONS -----------------------------------------------------\n",
    "# VIDEO: Creating a function named frame_to_time to convert frame number to time format \n",
    "def frame_to_time(frame, fps):\n",
    "    \"\"\"\n",
    "    frame_to_time converts a given frame number to a time format (HH:MM:SS.SS) based on the frames per second (fps).\n",
    "    Arguments:\n",
    "        frame (int): The frame number to be converted.\n",
    "        fps (float): The frames per second of the video.\n",
    "    Returns:\n",
    "        str: The time format as a string in the format \"HH:MM:SS.SS\".\n",
    "    \"\"\"\n",
    "    seconds = frame / fps\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:.2f}\"\n",
    "\n",
    "print(\"Function \\\"frame_to_time\\\" created sucesfully\") \n",
    "\n",
    "\n",
    "\n",
    "# Function to extract specified events (with correspodning LSL times) from XDF stream (useful for plotting)\n",
    "def get_events(stream, event_names):\n",
    "    \"\"\"\n",
    "    Extracts events and corresponding LSL times from the given stream that match any of the event_names.\n",
    "\n",
    "    Parameters:\n",
    "    stream (dict): The stream containing time stamps and event data.\n",
    "    event_names (list of str): List of event name substrings to look for in the events.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An array where each row contains a timestamp and the full event name.\n",
    "    \"\"\"\n",
    "    events = []  # Initialize an empty list to store matching events\n",
    "\n",
    "    # Check if the stream type is \"Markers\"\n",
    "    if stream['info']['type'][0] != \"Markers\":\n",
    "        raise ValueError(f\"ERROR: The stream provided ({stream['info']['name'][0]}) is not a Marker stream\")\n",
    "\n",
    "    # Iterate over the time stamps and corresponding events in the stream\n",
    "    for timestamp, event in zip(stream['time_stamps'], stream['time_series']):\n",
    "        # Check if any of the specified event names are in the current event\n",
    "        for name in event_names:\n",
    "            if name in event[0]:\n",
    "                # If a match is found, append the timestamp and full event name to the list\n",
    "                events.append([timestamp, event[0]])\n",
    "\n",
    "    # Convert the list of events to a NumPy array and return it\n",
    "    return np.array(events)\n",
    "\n",
    "print(\"Function \\\"get_events\\\" created sucesfully\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabef2f",
   "metadata": {},
   "source": [
    "## 2. Clipping Videos Based on .CSV frames and LSL times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the relevant CSV files that contain the LSL_time stamps and corresponding frame numbers\n",
    "for participant_frame, participant_video in participants_file_video_mapping:\n",
    "    \n",
    "    # Loop through the files in the input_file_folder adn Loadt the file_input_folder and selecting the (all) fles containing the participant_frame name\n",
    "    for file in os.listdir(os.path.abspath(input_file_folder)):\n",
    "        \n",
    "        if participant_frame in file and file.endswith ('csv'):   # Check if the participant_frame name is in the file name (and the file is a CSV file)\n",
    "            \n",
    "            print(f'Processing {participant_frame} for {participant_video}: {file}')\n",
    "            \n",
    "             # Load the CSV file using pandas\n",
    "            file_path = os.path.join(os.path.abspath(input_file_folder), file)  # Create the file path for the CSV file\n",
    "            file_data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Find the start and end frame numbers in the CSV file \n",
    "            start_frame = file_data.iloc[0, 1]    # Get the first frame (first row, index 0) in the second coluimn (index 1)\n",
    "            end_frame   = file_data.iloc[-1, 1]   # Get the last frame (last row, index -1) in the second coluimn (index 1)\n",
    "            \n",
    "            fnam = os.path.basename(file)[:-4]        # Extract the file name from the path and assings it to fnam, whilst removing the '.csv' extension (i.e., the last 4 characters in the string)\n",
    "            \n",
    "            # Find the frame rate of the LSL frames (i.e., the number of frames per second)\n",
    "            LSL_frame_rate = (end_frame - start_frame) / (file_data.iloc[-1, 0] - file_data.iloc[0, 0])  # Calculate the frame rate by dividing the number of frames by the time difference between the first and last frame\n",
    "            \n",
    "            # Load the corresponding raw video file using OpenCV\n",
    "            for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "                \n",
    "                if participant_video in video:  # Check if the participant_video name is in the video name\n",
    "                    \n",
    "                    print(f'Now Loading the Video :  {video}')\n",
    "                    \n",
    "                    video_path = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "                    \n",
    "                    video_extension = os.path.splitext(video_path)[1].lower()  # Extract the file extension from the video file path, ensuring it's in lowercase (e.g., '.mp4'), to enable case-insensitive matching in the dictionary\n",
    "                    \n",
    "                    capture = cv2.VideoCapture(video_path) # Load the video using OpenCV\n",
    "                    \n",
    "                    # Extract the relevant metadata from the video\n",
    "                    video_frame_width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))  \n",
    "                    video_frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))  \n",
    "                    video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n",
    "                    video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "                    \n",
    "                    # Check video extension format and select the appropriate codec in the dictionary\n",
    "                    if video_extension in extension_to_codec:\n",
    "                        codec = extension_to_codec[video_extension]\n",
    "                    else:\n",
    "                        raise ValueError(f\"ERROR: The video extension {video_extension} is not supported\")\n",
    "                    \n",
    "                    # Assign video extension to Four Character Code (fourcc) for the codec\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "                    \n",
    "                    # Create the output subfolder for the clipped videos\n",
    "                    subfolder_path = os.path.join(os.path.abspath(output_video_folder), f'{fnam}_clipped.{video_extension[1:]}')  # Create the output subfolder for the clipped video with the correct extension\n",
    "                    \n",
    "                    # Initialize the VideoWriter object to write frames to a new video file based on the fourcc codec, LSL frame rate, video frame size, and subfolder path\n",
    "                    out = cv2.VideoWriter(subfolder_path, fourcc, LSL_frame_rate, (video_frame_width, video_frame_height))  \n",
    "                    \n",
    "                    \n",
    "                    # Write the selected frames to the a new clipped video \n",
    "                    capture.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # Set the video capture object to the start frame\n",
    "                    with tqdm(total= end_frame - start_frame + 1,   desc=\"Rewriting Video Progress\", leave=False, ncols=100) as pbar:    # Create a progress bar for the video frames\n",
    "                        \n",
    "                        frame_count = start_frame  # Initialize the frame count to the start frame\n",
    "                        \n",
    "                        while capture.isOpened() and frame_count <= end_frame:  # Loop through the video frames from the start frame until the end frame is reached\n",
    "            \n",
    "                            try: \n",
    "                                ret, frame = capture.read()   # Read the next frame from the video\n",
    "                                if ret:   # If the frame is read correctly\n",
    "                                    frame_count += 1\n",
    "                                    pbar.update(1)\n",
    "                                    out.write(frame)  # Write the frame to the new video\n",
    "                                else:\n",
    "                                    break\n",
    "                                \n",
    "                            except Exception as e:  # Catch any errors that occur during the frame processing\n",
    "                                print(f\"An error occurred at frame {frame_count}: {e}\")\n",
    "                                break  # Stop processing if an error occurs\n",
    "                            \n",
    "                            \n",
    "                    # Release the video capture and video writer objects\n",
    "                    capture.release()\n",
    "                    out.release()\n",
    "                        \n",
    "                    print(f'Clipped video saved to: {subfolder_path}')\n",
    "                        \n",
    "                    print(\"\\n\")\n",
    "                    \n",
    "                    \n",
    "print(\"All videos have been clipped successfully. Look into your folder \" + output_video_folder) #as terminal            # Create the output subfolder for the clipped videos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906844e",
   "metadata": {},
   "source": [
    "## 3. Aligning (Clipped) Vidoes with (Clipped) Audios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34653809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P1_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_Movement_0_StartParticipantSinging_NoVision_Movement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_Movement_1_StartParticipantSinging_NoVision_Movement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_Movement_2_StartParticipantSinging_NoVision_Movement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_Movement_3_StartParticipantSinging_NoVision_Movement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_Movement_4_StartParticipantSinging_NoVision_Movement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_NoMovement_0_StartParticipantSinging_NoVision_NoMovement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_NoMovement_1_StartParticipantSinging_NoVision_NoMovement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_NoMovement_2_StartParticipantSinging_NoVision_NoMovement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_NoMovement_3_StartParticipantSinging_NoVision_NoMovement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_NoVision_NoMovement_4_StartParticipantSinging_NoVision_NoMovement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_Movement_0_StartParticipantSinging_Vision_Movement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_Movement_1_StartParticipantSinging_Vision_Movement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_Movement_2_StartParticipantSinging_Vision_Movement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_Movement_3_StartParticipantSinging_Vision_Movement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging._denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging._denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_Movement_4_StartParticipantSinging_Vision_Movement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_NoMovement_0_StartParticipantSinging_Vision_NoMovement_0_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_NoMovement_1_StartParticipantSinging_Vision_NoMovement_1_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_NoMovement_2_StartParticipantSinging_Vision_NoMovement_2_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_NoMovement_3_StartParticipantSinging_Vision_NoMovement_3_EndParticipantSinging_audiovideo_synced.avi\n",
      "Processing the audio file:  T1_experiment_Mic_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Found matching video: T1_experiment_Video_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_clipped.avi for audio: T1_experiment_Mic_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_denoised.wav\n",
      "Now combining this Audio and Video\n",
      "Video with audio saved as c:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\1_XDF_PROCESSING\\audiovideo_sync_clipped\\T1\\marker_MULTIPLEpairs\\_P2_Vision_NoMovement_4_StartParticipantSinging_Vision_NoMovement_4_EndParticipantSinging_audiovideo_synced.avi\n",
      "Done, you can now look into the folder: ./audiovideo_sync_clipped/T1/marker_MULTIPLEpairs\n"
     ]
    }
   ],
   "source": [
    "input_video_folder = './video_clipped/T1/marker_MULTIPLEpairs'     # this folder contains the clipped videos\n",
    "\n",
    "# LOADING AUDIOS: Loop over files in the input_file_folder to extract the relevant audio files\n",
    "for audio in os.listdir(os.path.abspath(input_file_folder)):\n",
    "        \n",
    "    if 'Mic' in audio and 'denoised' in audio and audio.endswith('wav'):  # Correct check for 'Mic' and 'denoised'\n",
    "        \n",
    "        print(f'Processing the audio file:  {audio}')\n",
    "        \n",
    "        audio_path = os.path.join(os.path.abspath(input_file_folder), audio)\n",
    "        \n",
    "        # Extract the identifier from the audio file name that will be used to match the video file\n",
    "        audio_match = audio_video_pattern_matching.search(audio)\n",
    "        if not audio_match:\n",
    "            # print(f'No matching for audio: {audio}')\n",
    "            continue  # Skip current iteration if the pattern is not found\n",
    "        \n",
    "        audio_identifier = audio_match.group(0)  # Extract the matched portion of the string from the regex result.\n",
    "        #print(f'Audio identifier: {audio_identifier}')  # Print the matched identifier for audio\n",
    "\n",
    "\n",
    "        # LOADING VIDEOS: Loop over video files to select the corresponding video file\n",
    "        for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "            \n",
    "            # Extract the identifier from the video file name that will be used to match the audio file\n",
    "            video_match = audio_video_pattern_matching.search(video)\n",
    "            if not video_match:\n",
    "                #print(f'No matching for video: {video}')\n",
    "                continue  # Skip current iteration if the pattern is not found\n",
    "            \n",
    "            video_identifier = video_match.group(0)  # Extract the matched portion of the string from the regex result.\n",
    "            #print(f'Video identifier: {video_identifier}')  # Print the matched identifier for video\n",
    "\n",
    "     \n",
    "            # Check if the identifiers from the audio and video files match\n",
    "            if audio_identifier == video_identifier: \n",
    "                \n",
    "                video_path = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "                video_extension = os.path.splitext(video_path)[1].lower()  # Ensure correct video extension\n",
    "                \n",
    "                print(f'Found matching video: {video} for audio: {audio}')\n",
    "                \n",
    "                # Create output folder if it doesn't exist\n",
    "                os.makedirs(output_audiovideo_folder, exist_ok=True)\n",
    "                \n",
    "                # ALIGNMENT: Combining Audio and Video using ffmpeg \n",
    "                subfolder_path = os.path.join(os.path.abspath(output_audiovideo_folder), f'{video_identifier}_audiovideo_synced.{video_extension[1:]}')\n",
    "                \n",
    "                # Construct the ffmpeg command to combine the audio and video files\n",
    "                ffmpeg_command = [\n",
    "                    'ffmpeg',              # Call the ffmpeg tool\n",
    "                    '-y',                  # Overwrite the output file without asking\n",
    "                    '-i', video_path,      # Specify the input video file\n",
    "                    '-i', audio_path,      # Specify the input audio file\n",
    "                    '-c:v', 'copy',        # Copy the video stream without re-encoding\n",
    "                    '-c:a', 'aac',         # Re-encode the audio stream to AAC format (optimized for playback devices)\n",
    "                    '-strict', 'experimental',  # Enable experimental features (needed for AAC encoding in some versions)\n",
    "                    subfolder_path         # Specify the output file path (video with audio)\n",
    "                ]\n",
    "                \n",
    "                # Run the command \n",
    "                print('Now combining this Audio and Video')\n",
    "                try:\n",
    "                    result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "                    #print(result.stdout)\n",
    "                    \n",
    "                    print(f'Video with audio saved as {subfolder_path}')\n",
    "                    \n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error combining audio and video {video_path} and {audio_path}: {e.stderr}\")\n",
    "                     \n",
    "                \n",
    "print(f'Done, you can now look into the folder: {output_audiovideo_folder}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
