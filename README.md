# MobileMultimodalLab
### *An Open-Source, Low-Cost and Portable Laboratory for the study of Multimodal Human Behavior*

![Mobile Multimodal Lab Logo](Donders_MML_LOGO.png)

MobileMultimodalLab is a project initiated by researchers at Donders Center for Cognition. It aims to provide a lab setup for anyone interested in studying multimodal interactive behaviour - including acoustics, body movement, muscle activity, eye movement, etc - with as little costs as possible. 

To achieve this, we are working on a comprehensive coding library, accompanied by a practical manual, that shall help researchers to build their own MobileMultimodalLab. Our guiding principles are:
- **Open-source resources** - All code and documentation is freely available to everyone
- **Low-cost equipment** - We want to build the setup with as little monetary cost as possible (i.e., less than 10K)
- **Portable setup** - The setup should be easily transportable across locations

The MML setup consists of
- **multiple frame-synced 2D cameras** that allow for 3D motion tracking
- **multiple microphones** for acoustic analysis
- **multiple physiological sensors** for measuring heart rate, muscle activity, and respiration


            ADD FIGURE OF SETUP

Additionally, the setup is build in a modular way, so that anyone can remove sensors they do not need, or potentially add new ones that the default version does not include.

To ensure that all the signals are synchronized, we use the Lab Stream Layer (ADD LINK), a software that synchronizes different data streams with sub-millisecond precision, crucially simplifying the data collection process and subsequent processing.

## Repository structure

In this repository, you will find
- Scripts for recording and processing multimodal behaviour
- ...
- ...

## Measuring equipment
Until now, the following equipment is documented:
- table with equipment

## How to get started

## How to contribute