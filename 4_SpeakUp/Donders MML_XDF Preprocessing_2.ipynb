{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f4587d",
   "metadata": {},
   "source": [
    "# Donders MML: XDF processing\n",
    "The script processes all data files sotred in an XDF format. \n",
    "Steps: \n",
    "1. Import libraries necessary for processing audio, video and data files \n",
    "    a. See requirements.txt to conda install all the necessary packages. \n",
    "    \n",
    "2. Identify XDF files within a specified directory or its subdirectories.\n",
    "\n",
    "3. ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceca857",
   "metadata": {},
   "source": [
    "## 0. Import all the necessary packages to work with XDF, Audio and Video files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f88e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything was imported succesfully\n"
     ]
    }
   ],
   "source": [
    "import os  # Importing the os module which provides functions for interacting with the operating system\n",
    "import pyxdf  # Importing pyxdf, a Python library for reading XDF files\n",
    "import glob  # Importing the glob module which helps in finding files/directories with specific patterns\n",
    "import pandas as pd  # Importing pandas library (abbreviated as pd), which is used for data manipulation and analysis\n",
    "import numpy as np  # Importing numpy library (abbreviated as np), which is used for numerical computations\n",
    "import wave  # Importing wave module for reading and writing WAV files (usually audio files) \n",
    "import struct  # Importing struct module which provides functions to convert between Python values and C structs\n",
    "import math  # Importing math module which provides mathematical functions\n",
    "import random  # Importing random module for generating random numbers\n",
    "from scipy.io import wavfile  # Importing wavfile module from scipy.io (a library built on numpy), for reading and writing WAV files\n",
    "import noisereduce as nr  # Importing noisereduce module for noise reduction in audio signals\n",
    "import json  # Importing json module for working with JSON data\n",
    "import cv2  # Importing OpenCV library for computer vision tasks\n",
    "from moviepy.editor import (  # Importing various classes and functions from moviepy.editor module\n",
    "                            VideoFileClip,  # Class for working with video files\n",
    "                            AudioFileClip,  # Class for working with audio files\n",
    "                            CompositeAudioClip)  # Class for composing audio clip\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip # video  clipping fucntion \n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip # alternative video clipping function\n",
    "import matplotlib.pyplot as plt  # Importing pyplot library to create figures and plot data \n",
    "from matplotlib.widgets import Slider  \n",
    "import tkinter # GUI toolkit to open and save files\n",
    "from tkinter import filedialog # GUI toolkit to open and save files\n",
    "import subprocess \n",
    "#import ffmpeg     # Question about this\n",
    "# import xdf\n",
    "\n",
    "print(\"Everything was imported succesfully\") #as terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb71e5f-010b-44c3-8ce2-850514916a35",
   "metadata": {},
   "source": [
    "## 1. Define the Relevant Paths, Variables & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb3769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input folder = C:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\4_SpeakUp\\data_raw\n",
      "Output folder = C:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\4_SpeakUp\\data_processed\n",
      "Function \"to_audio\" created sucesfully\n",
      "Function \"frame_to_time\" created sucesfully\n",
      "Function \"save_xdf\" created sucesfully\n",
      "Function \"rename_streams\" created sucesfully\n",
      "Function \"clip_nonmarker_streams\" created successfully\n",
      "Function \"get_events\" created sucesfully\n"
     ]
    }
   ],
   "source": [
    "# ------------ PATHS -----------------------------------------------------\n",
    "input_folder = './data_raw/'  # input folder with the raw XDF files (relative path) \n",
    "output_folder = './data_processed/'  # output folder where the raw extracted data will be saved (relative path) \n",
    "\n",
    "print(\"Input folder =\", os.path.abspath(input_folder))\n",
    "print(\"Output folder =\", os.path.abspath(output_folder))\n",
    "\n",
    "\n",
    "# ------------ VARIABLES ----------------------------------------------\n",
    "noise_reducelevel = 1.5  #This can be changed accordingly \n",
    "\n",
    "\n",
    "# Dictionary to map file extensions to codecs\n",
    "extension_to_codec = {\n",
    "    '.mp4': 'libx264',\n",
    "    '.avi': 'libxvid',\n",
    "    '.mov': 'libx264',\n",
    "    '.mkv': 'libx264',\n",
    "    '.flv': 'flv',\n",
    "    # Add more mappings as needed\n",
    "                    }\n",
    "\n",
    "\n",
    "# IF NEEDED: Create a dictionary mapping from old stream names to new stream names (# Edit and add more mappings as needed.) \n",
    "     # (This dictionary mapping is based both of the stream_names and stream_types because in our case we have 2 streams with the same name (but different types)) \n",
    "rename_dict = {\n",
    "    ('MyWebcamFrameStream_2', 'frameNR'): 'Video_P2',\n",
    "    ('MyWebcamFrameStream_1', 'frameNR'): 'Video_P1',\n",
    "    ('Mic', 'voice'): 'Mic_P1',\n",
    "    ('Mic_004', 'voice'): 'Mic_P2',\n",
    "    ('OpenSignals', '00:07:80:8C:06:6A'): 'PLUX_P2',\n",
    "    ('OpenSignals', '00:07:80:D8:A8:81'): 'PLUX_P1'\n",
    "}\n",
    "\n",
    "\n",
    "# -------------FUNCTIONS------------------------------------------------------------------------------------\n",
    "# AUDIO: Creating a function named \"to_audio\" tht writes audio data (input) and transforms into a WAV file (output). \n",
    "def to_audio(fileloc, timeseries_name, samplerate = 16000, channels = 1):   \n",
    "    \"\"\"\n",
    "    This function - named \"to_audio\" - writes audio data to a WAV file.\n",
    "    It accepts the following parameters:\n",
    "    - fileloc (str): Location to save the audio file.\n",
    "    - timeseriestype (list): Audio data to be written into the file.\n",
    "    - samplerate (int, optional): Sampling rate of the audio data. Defaults to 16000.\n",
    "    - channels (int, optional): Number of audio channels (mono or stereo). Defaults to 1 (mono)\n",
    "    \"\"\"\n",
    "    if 'Mic' in timeseries_name:  #Condition check that the timeseriestype belongs to the microphone.\n",
    "            \n",
    "        obj = wave.open(fileloc,'w')        # Opens audio file using the wave.open() function write mode ('w'). Assigns data it to the variable obj.\n",
    "        obj.setnchannels(channels)          # Sets the number of channels in the audio file using obj.setnchannels(channels). Deafault 1 channel (mono).\n",
    "        obj.setsampwidth(2)                 # Sets the sample width in bytes using obj.setsampwidth(2). The value '2' indicates 16-bit audio.\n",
    "        obj.setframerate(float(samplerate)) # sets the frame rate of the audio file using obj.setframerate(float(samplerate)), where samplerate is provided as a parameter.\n",
    "            \n",
    "        for i in timeseries:                      # Loop to iterate over each time-point in the temeseries stream\n",
    "            data = struct.pack('<h', int(i[0]))   # Converts the first value of the timeseries to an integer and packs it into a binary string (struck.pack()) according to the '<h' fromat (i.e., short integer (16 bits) in little-endian byte order)   \n",
    "            obj.writeframesraw( data )            # Writes the packed binary data into an audio file using the wave function writeframesraw() from the wave library \n",
    "        obj.close()                               # Closes the audio file \n",
    "\n",
    "print(\"Function \\\"to_audio\\\" created sucesfully\") \n",
    "\n",
    "\n",
    "# VIDEO: Creating a function named frame_to_time to convert frame number to time format \n",
    "def frame_to_time(frame, fps):\n",
    "    \"\"\"\n",
    "    frame_to_time converts a given frame number to a time format (HH:MM:SS.SS) based on the frames per second (fps).\n",
    "    Arguments:\n",
    "        frame (int): The frame number to be converted.\n",
    "        fps (float): The frames per second of the video.\n",
    "    Returns:\n",
    "        str: The time format as a string in the format \"HH:MM:SS.SS\".\n",
    "    \"\"\"\n",
    "    seconds = frame / fps\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:.2f}\"\n",
    "\n",
    "print(\"Function \\\"frame_to_time\\\" created sucesfully\") \n",
    "\n",
    "\n",
    "# XDF Save \n",
    "def save_xdf(filename, streams, header):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pyxdf.write_header(f, header)\n",
    "        for stream in streams:\n",
    "            pyxdf.write_stream_header(f, stream['info'])\n",
    "            pyxdf.write_stream_data(f, stream['time_stamps'], stream['time_series'])\n",
    "\n",
    "print(\"Function \\\"save_xdf\\\" created sucesfully\") \n",
    "\n",
    "\n",
    "# Renaming XDF Stream (if necessary)\n",
    "def rename_streams(streams, rename_dict):\n",
    "    \"\"\"\n",
    "    Function rename_stream renames any streams based on the rename dictionary (if name found in remane_dict)\n",
    "        Parameters:\n",
    "    stream_name (str): The current name of the stream.\n",
    "    stream_type (str): The type of the stream.\n",
    "    rename_dict (dict): A dictionary mapping old stream names and types to new stream names.\n",
    "        Returns:\n",
    "    str: The new stream name if found in rename_dict, otherwise the original stream name.\n",
    "    \"\"\"\n",
    "    for stream in streams:\n",
    "        stream_name = stream['info']['name'][0]\n",
    "        stream_type = stream['info']['type'][0]\n",
    "\n",
    "        if (stream_name, stream_type) in rename_dict:\n",
    "            new_name = rename_dict[(stream_name, stream_type)]\n",
    "            print(f'Renaming stream {stream_name} ({stream_type}) to {new_name}')\n",
    "            stream['info']['name'][0] = new_name  # Rename the stream\n",
    "    return streams\n",
    "\n",
    "print(\"Function \\\"rename_streams\\\" created sucesfully\") \n",
    "\n",
    "\n",
    "# Function to clip the streams\n",
    "def clip_nonmarker_streams(streams):\n",
    "    \"\"\"\n",
    "    Function clip_nonmarker_streams cuts the start and end of all streams that are not \"M<arkers\" type in an XDF file based on the shortest stream.\n",
    "    \n",
    "    Input: \n",
    "        streams: list of streams to be clipped latest_start_time and earliest_end_time of all streams except Markers ones\n",
    "    Output: \n",
    "        clipped streams based on the \n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out marker streams for calculating the latest start and earliest end times\n",
    "    non_marker_streams = [stream for stream in streams if stream['info']['type'][0] != 'Markers']\n",
    "    \n",
    "    # Find the latest start time across all non-marker streams\n",
    "    latest_start_time = max([stream['time_stamps'][0] for stream in non_marker_streams])  # Get the first timestamp of each non-marker stream and find the maximum (latest start time)\n",
    "    \n",
    "    # Find the earliest end time across all non-marker streams\n",
    "    earliest_end_time = min([stream['time_stamps'][-1] for stream in non_marker_streams])  # Get the last timestamp of each non-marker stream and find the minimum (earliest end time)\n",
    "\n",
    "    clipped_streams = []  # Initialize an empty list to store the clipped streams\n",
    "\n",
    "    for stream in streams:\n",
    "        time_stamps = np.array(stream['time_stamps'])  # Convert the timestamps to a NumPy array\n",
    "        time_series = np.array(stream['time_series'])  # Convert the time series data to a NumPy array\n",
    "\n",
    "        # Find the index of the closest timestamp to the latest start time\n",
    "        start_idx = np.searchsorted(time_stamps, latest_start_time, side='left')  # Get the index where the latest start time would fit\n",
    "        # Ensure the index is within the valid range\n",
    "        start_idx = max(0, min(start_idx, len(time_stamps) - 1))\n",
    "\n",
    "        # Find the index of the closest timestamp to the earliest end time\n",
    "        end_idx = np.searchsorted(time_stamps, earliest_end_time, side='right')  # Get the index where the earliest end time would fit\n",
    "        # Ensure the index is within the valid range\n",
    "        end_idx = max(0, min(end_idx, len(time_stamps)))\n",
    "\n",
    "        # Clip the timestamps array to the range between the found indices\n",
    "        clipped_time_stamps = time_stamps[start_idx:end_idx]  # Select the time stamps within the clipped range\n",
    "        # Clip the time series data array to the same range\n",
    "        clipped_time_series = time_series[start_idx:end_idx]  # Select the time series data within the clipped range\n",
    "\n",
    "        # Create a copy of the original stream dictionary\n",
    "        clipped_stream = stream.copy()  # Copy the stream dictionary\n",
    "        # Replace the timestamps and time series data with the clipped versions\n",
    "        clipped_stream['time_stamps'] = clipped_time_stamps  # Update the timestamps with the clipped data\n",
    "        clipped_stream['time_series'] = clipped_time_series  # Update the time series with the clipped data\n",
    "\n",
    "        clipped_streams.append(clipped_stream)  # Add the clipped stream to the list\n",
    "\n",
    "    return clipped_streams  # Return the list of clipped streams\n",
    "\n",
    "print(\"Function \\\"clip_nonmarker_streams\\\" created successfully\") \n",
    "\n",
    "\n",
    "# Function to extract specified events (with correspodning LSL times) from XDF stream (useful for plotting)\n",
    "def get_events(stream, event_names):\n",
    "    \"\"\"\n",
    "    Extracts events and corresponding LSL times from the given stream that match any of the event_names.\n",
    "\n",
    "    Parameters:\n",
    "    stream (dict): The stream containing time stamps and event data.\n",
    "    event_names (list of str): List of event name substrings to look for in the events.\n",
    "\n",
    "    Returns:\n",
    "    np.array: An array where each row contains a timestamp and the full event name.\n",
    "    \"\"\"\n",
    "    events = []  # Initialize an empty list to store matching events\n",
    "\n",
    "    # Check if the stream type is \"Markers\"\n",
    "    if stream['info']['type'][0] != \"Markers\":\n",
    "        raise ValueError(f\"ERROR: The stream provided ({stream['info']['name'][0]}) is not a Marker stream\")\n",
    "\n",
    "    # Iterate over the time stamps and corresponding events in the stream\n",
    "    for timestamp, event in zip(stream['time_stamps'], stream['time_series']):\n",
    "        # Check if any of the specified event names are in the current event\n",
    "        for name in event_names:\n",
    "            if name in event[0]:\n",
    "                # If a match is found, append the timestamp and full event name to the list\n",
    "                events.append([timestamp, event[0]])\n",
    "\n",
    "    # Convert the list of events to a NumPy array and return it\n",
    "    return np.array(events)\n",
    "\n",
    "print(\"Function \\\"get_events\\\" created sucesfully\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3587c",
   "metadata": {},
   "source": [
    "## 2. Identifying XDF files in Input Folder or any Subfolder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fd9537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have idenified the following XDF files: []\n"
     ]
    }
   ],
   "source": [
    "xdf_files = []  # Initialize an empty list to store paths of XDF files\n",
    "\n",
    "# Traverse through the directory and its subdirectories to find XDF files\n",
    "for root, dirs, files in os.walk(input_folder):  # 1st loop iterating over the results returned by os.walk().\n",
    "    \n",
    "    for file in files:                                    # 2nd loop iterating through each file in the current directory\n",
    "        \n",
    "        if file.endswith(\".xdf\"):                         # checking if the file has and XDF extension \n",
    "            \n",
    "             xdf_files.append(os.path.join(root, file))   # if the file is an XDF file, append its full path to the xdf_files list\n",
    "            \n",
    "print('We have idenified the following XDF files: ' + str(xdf_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd34ee1-0a3c-4fff-aba6-2213040dc729",
   "metadata": {},
   "source": [
    "## 2a. Alternatively, the user can select their own XDF file from any Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4f83bf-1184-46c1-941f-accfe5dfbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected the following XDF files: ['C:/Users/ahmar/OneDrive/Documents/GitHub/Mobile-Multimodal-Lab/4_SpeakUp/data_raw/pilot_01_d2_speech_lsl.xdf']\n"
     ]
    }
   ],
   "source": [
    "import tkinter # GUI toolkit to open and save files\n",
    "from tkinter import filedialog # GUI toolkit to open and save files\n",
    "\n",
    "root = tkinter.Tk()\n",
    "root.attributes('-topmost',True)\n",
    "root.iconify()\n",
    "\n",
    "xdf_files = filedialog.askopenfilename(title=\"Select an XDF file\", filetypes=[(\"XDF Files\", \"*.xdf\")], multiple = 'True')\n",
    "\n",
    "root.destroy()\n",
    "\n",
    "# Convert the tuple returned by askopenfilenames() to a list\n",
    "xdf_files = list(xdf_files)\n",
    "\n",
    "print('You have selected the following XDF files: ' + str(xdf_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9c679-7d1c-4c5b-8962-c5f1ea6d871c",
   "metadata": {},
   "source": [
    "# 3. (NEW) Main Loop that Extracts & RENAMES each data stream from each XDF and saves it as CVS or WAV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df87847-8651-4e9e-a681-a48534cdd25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading xdf file: C:/Users/ahmar/OneDrive/Documents/GitHub/Mobile-Multimodal-Lab/4_SpeakUp/data_raw/pilot_01_d2_speech_lsl.xdf\n",
      "working on stream: pupil_labs_Event  with a channel count of: 1 labelled: ['Channel 1'] and a sampling rate of 0\n",
      "Saving: pilot_01_d2_speech_lsl_pupil_labs_Event\n",
      "working on stream: MyMarkerStream  with a channel count of: 1 labelled: ['Channel 1'] and a sampling rate of 0\n",
      "Saving: pilot_01_d2_speech_lsl_MyMarkerStream\n",
      "working on stream: pupil_labs_Gaze  with a channel count of: 2 labelled: ['x', 'y'] and a sampling rate of 0\n",
      "Saving: pilot_01_d2_speech_lsl_pupil_labs_Gaze\n",
      "working on stream: tACS_triggers  with a channel count of: 1 labelled: ['Channel 1'] and a sampling rate of 0\n",
      "Saving: pilot_01_d2_speech_lsl_tACS_triggers\n",
      "working on stream: OpenSignals  with a channel count of: 3 labelled: ['nSeq', 'EDA0', 'ECG1'] and a sampling rate of 1000\n",
      "Saving: pilot_01_d2_speech_lsl_OpenSignals\n",
      "working on stream: MyWebcamFrameStream  with a channel count of: 1 labelled: ['Channel 1'] and a sampling rate of 500\n",
      "Saving: pilot_01_d2_speech_lsl_MyWebcamFrameStream\n",
      "working on stream: BalanceBoard_stream  with a channel count of: 4 labelled: ['Channel 1', 'Channel 2', 'Channel 3', 'Channel 4'] and a sampling rate of 500\n",
      "Saving: pilot_01_d2_speech_lsl_BalanceBoard_stream\n",
      "working on stream: Mic  with a channel count of: 1 labelled: ['Channel 1'] and a sampling rate of 16000\n",
      "Saving: pilot_01_d2_speech_lsl_Mic\n",
      "Done with extracting all the streams! You can now look into your folder: ./data_processed/\n"
     ]
    }
   ],
   "source": [
    "output_folder = './data_processed/'  # output_folder for the CVS and WAV files \n",
    "\n",
    "for xdf_file in xdf_files:               # Iterate over each path in the list \"xdf_files\". \n",
    "\n",
    "    print('loading xdf file: ' + xdf_file )\n",
    "\n",
    "    streams, header = pyxdf.load_xdf(xdf_file)    # Loading steams and header information from each XDF file usign the load_xdf function from the pyxdf library. \n",
    "    fnam = os.path.basename(xdf_file)[:-4]        # Extract the file name from the path and assings it to fnam, whilst removing the '.xdf' extension (i.e., the last 4 characters in the string)\n",
    "       \n",
    "    # Renaming streams using the rename_dict and rename_function (if names match dictionary)\n",
    "    # streams = rename_streams(streams, rename_dict)\n",
    "\n",
    "    # Clipping the streams based on the latest_start_time and earliest_end_time of (any of the) streams in the xdf file. \n",
    "    # streams = clip_nonmarker_streams(streams)\n",
    "\n",
    "    # Navigating through each stream and extracting the relevant information (e.g., name, sample rate, data, time, etc.) \n",
    "    for stream in streams:                        # Iterate over each steam in the loaded steam for each XDF file .\n",
    "        timeseries_name = stream['info']['name'][0]                    # Extracts name of each steams (dictionary structure) and assigns it to timeseries_name (e.g., Mic). \n",
    "        timeseries_type = stream['info']['type'][0]\n",
    "        timevec = stream['time_stamps']                                # Extract the time_stamps (i.e.,LSL TIMESTAMPS) from the \"stream\" dictionary and assigns it to the variable timevec\n",
    "        timeseries = stream['time_series']                              # Extract the time_series (i.e., DATA) from the \"stream\" dictionary and \n",
    "        samplerate = round(float(stream['info']['nominal_srate'][0]))    # Extracts the rounded sampling rate (nominal_srate) and assings it to samplerate \n",
    "        channelcount = int(stream['info']['channel_count'][0])               # Extracts the number of channel for each steams and assigns it to channelcount (as an integer)\n",
    "\n",
    "        # Extract channel labels (this step is a bit more complex because of the XDF file configuration)\n",
    "        channel_labels = []  # Initialize an empty list to store channel labels\n",
    "        \n",
    "        # Check if there is more than one channel and if the 'desc' field is present in the stream's info\n",
    "        if channelcount > 1 and stream['info']['desc'] is not None:\n",
    "            try:\n",
    "                channels_info = stream['info']['desc'][0]['channels'][0]['channel']  # Attempt to extract channel information from the stream's description\n",
    "                channel_labels = [channel['label'][0] for channel in channels_info]  # Create a list of channel labels by extracting the 'label' field from each channel's info\n",
    "            except (KeyError, IndexError):\n",
    "                channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]  # If there is any issue with extracting channel labels, create default labels\n",
    "        else:\n",
    "            channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]  # If there is only one channel or the 'desc' field is not present, create default labels\n",
    "        \n",
    "        print('working on stream: ' + timeseries_name + '  with a channel count of: ' + str(channelcount) + ' labelled: ' + str(channel_labels) + ' and a sampling rate of ' + str(samplerate))        \n",
    "\n",
    "        # Saving each stream of the XDF file as a CSV (if it doesn't exist yet) \n",
    "        matrix_aux = np.vstack([np.transpose(timevec),np.transpose(timeseries)])   # Create matrix_aux by concatenating the transposed timevec and timeseries\n",
    "        matrix     = np.transpose(matrix_aux)                                      # Create new matrix by tranposing matrix_aux\n",
    "        column_names = [\"LSL_Time\"] + [f\"{timeseries_name}_{label}\" for label in channel_labels]  # Create column names for the matrix. The first column is named \"LSL_Time\" (i.e., timevec), The subsequent columns are named using the format {timeseries_name}_{label} for each channel label. \n",
    "        df_lab = pd.DataFrame(matrix, columns = column_names)                                     # Create a DataFrame df_lab with the combined data and the appropriate column names.\n",
    "       \n",
    "        # Saving \n",
    "        print('Saving: ' + fnam + '_' + timeseries_name) \n",
    "        df_lab.to_csv(output_folder + fnam + '_' + timeseries_name + '.csv',index=False)  \n",
    "      \n",
    "\n",
    "        if \"Mic\" in timeseries_name:  # Check if the data stream is from a microphone\n",
    "            wavloc = os.path.abspath(output_folder + fnam + '_' + timeseries_name  + '.wav')  # Define the location to save the initial audio file\n",
    "            to_audio(wavloc, timeseries_name)  # Convert the time series data to an audio file and save it at the defined location\n",
    "            rate, data = wavfile.read(wavloc)  # Load the audio data from the saved WAV file\n",
    "            reduced_noise = nr.reduce_noise(y=data, sr=rate, n_std_thresh_stationary=noise_reducelevel, stationary=True)    # Perform noise reduction based on the noise_reducelevel \n",
    "            wavloc2 = os.path.abspath(output_folder + fnam + '_' + timeseries_name + '_denoised.wav')  # Define the location to save the noise-reduced audio file\n",
    "            wavfile.write(wavloc2, rate, reduced_noise)  # Save the noise-reduced audio data as a new WAV file at the defined location\n",
    "\n",
    "print(\"Done with extracting all the streams! You can now look into your folder: \" + output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601313d8-f244-457a-ba37-2a6464f6a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61a32516",
   "metadata": {},
   "source": [
    "# 3. Main Loop that Extracts each data stream from each XDF file and saves as CVS or WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8f08c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_folder = './data_processed/'  # input folder with the raw XDF files (relative path) \n",
    "event_names = ['_StartParticipantSinging', '_EndParticipantSinging']\n",
    "\n",
    "for xdf_file in xdf_files:               # Iterate over each path in the list \"xdf_files\". \n",
    "\n",
    "    print('loading xdf file: ' + xdf_file )\n",
    "\n",
    "    streams, header = pyxdf.load_xdf(xdf_file)    # Loading steams and header information from each XDF file usign the load_xdf function from the pyxdf library. \n",
    "    fnam = os.path.basename(xdf_file)[:-4]        # Extract the file name from the path and assings it to fnam, whilst removing the '.xdf' extension (i.e., the last 4 characters in the string)\n",
    "       \n",
    "    stream_count = {}   # Dictionary to keep track of multiple streams with the same name (this can happen in cases of multiple people) \n",
    "\n",
    "    # Navigating through each stream and extracting the relevant information (e.g., name, sample rate, data, time, etc.) \n",
    "    for stream in streams:                        # Iterate over each steam in the loaded steam for each XDF file .\n",
    "        timeseries_name = stream['info']['name'][0]                    # Extracts name of each steams (dictionary structure) and assigns it to timeseriestype (e.g., Mic). \n",
    "        timevec = stream['time_stamps']                                # Extract the time_stamps (i.e.,LSL TIMESTAMPS) from the \"stream\" dictionary and assigns it to the variable timevec\n",
    "        timeseries = stream['time_series']                              # Extract the time_series (i.e., DATA) from the \"stream\" dictionary and \n",
    "        samplerate = round(float(stream['info']['nominal_srate'][0]))    # Extracts the rounded sampling rate (nominal_srate) and assings it to samplerate \n",
    "        channelcount = int(stream['info']['channel_count'][0])               # Extracts the number of channel for each steams and assigns it to channelcount (as an integer)\n",
    "\n",
    "        # Extract channel labels\n",
    "        channel_labels = []\n",
    "        if channelcount > 1 and stream['info']['desc'] is not None:\n",
    "            try:\n",
    "                channels_info = stream['info']['desc'][0]['channels'][0]['channel']\n",
    "                channel_labels = [channel['label'][0] for channel in channels_info]\n",
    "            except (KeyError, IndexError):\n",
    "                # If there is any issue with extracting channel labels, create default labels\n",
    "                channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]\n",
    "        else:\n",
    "            # If channelcount is 1 and/or'desc' is None, create default labels\n",
    "            channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]\n",
    "       \n",
    "        # Incrementing the count if the name of two (or more) streams are the same. \n",
    "        if timeseries_name in stream_count:\n",
    "            stream_count[timeseries_name] += 1\n",
    "        else:\n",
    "            stream_count[timeseries_name] = 1 \n",
    "        count = stream_count[timeseries_name]  # Get the current count for the stream type\n",
    "        \n",
    "        print('working on stream: ' + timeseries_name + '  with a channel count of: ' + str(channelcount) + ' labelled: ' + str(channel_labels) + ' and a sampling rate of ' + str(samplerate))        \n",
    "\n",
    "        # Saving each stream of the XDF file as a CSV (if it doesn't exist yet) \n",
    "        matrix_aux = np.vstack([np.transpose(timevec),np.transpose(timeseries)])   # Create matrix_aux by concatenating the transposed timevec and timeseries\n",
    "        matrix     = np.transpose(matrix_aux)                                      # Create new matrix by tranposing matrix_aux\n",
    "        column_names = [\"LSL_Time\"] + [f\"{timeseries_name}_{label}\" for label in channel_labels]  # Create column names for the matrix. The first column is named \"LSL_Time\" (i.e., timevec), The subsequent columns are named using the format {timeseries_name}_{label} for each channel label. \n",
    "        df_lab = pd.DataFrame(matrix, columns = column_names)                                     # Create a DataFrame df_lab with the combined data and the appropriate column names.\n",
    "        if count == 1: # Saving without adding count to the file name\n",
    "            df_lab.to_csv(output_folder + fnam + '_' + timeseries_name + '_nominal_srate' + str(samplerate)  + '.csv',index=False)  \n",
    "        else:         # Saving by adding count ot the file name (because of multiple streams with same name) \n",
    "            df_lab.to_csv(output_folder + fnam + '_' + timeseries_name + '_nominal_srate' + str(samplerate) + '_' + str(count) + '.csv',index=False)     #Saving the df_lab as CSV file named [fnam]_[timeseriestype]_[nomilar_srate#].cvs\n",
    "       \n",
    "\n",
    "        if \"Mic\" in timeseries_name:  # Check if the data stream is from a microphone\n",
    "            wavloc = os.path.abspath(output_folder + fnam + '_' + timeseries_name + '_nominal_srate' + str(samplerate) + '.wav')  # Define the location to save the initial audio file\n",
    "            to_audio(wavloc, timeseries_name)  # Convert the time series data to an audio file and save it at the defined location\n",
    "            rate, data = wavfile.read(wavloc)  # Load the audio data from the saved WAV file\n",
    "            reduced_noise = nr.reduce_noise(y=data, sr=rate, n_std_thresh_stationary=noise_reducelevel, stationary=True)    # Perform noise reduction based on the noise_reducelevel \n",
    "            wavloc2 = os.path.abspath(output_folder + fnam + '_' + timeseries_name + '_nominal_srate' + str(samplerate) + '_denoised.wav')  # Define the location to save the noise-reduced audio file\n",
    "            wavfile.write(wavloc2, rate, reduced_noise)  # Save the noise-reduced audio data as a new WAV file at the defined location\n",
    "\n",
    "print(\"Done with extracting all the streams! You can now look into your folder: \" + output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aab416-ccba-4bfd-9152-184b1918145b",
   "metadata": {},
   "source": [
    "## 4. Plotting Each XDF and Each Stream (Quality Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74810836-9beb-4d55-8ba3-4820b1ce5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selector_buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d41211-a5a5-4c23-b6d3-8ee71333055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "event_names = ['_StartParticipantSinging', '_EndParticipantSinging']\n",
    "\n",
    "\n",
    "# Navigating through each stream and extracting the relevant information (e.g., name, sample rate, data, time, etc.)\n",
    "for stream in streams:  # Iterate over each steam in the loaded steam for each XDF file.\n",
    "    timeseries_name = stream['info']['name'][0]  # Extracts name of each steams (dictionary structure) and assigns it to timeseries_name (e.g., Mic).\n",
    "    timeseries_type = stream['info']['type'][0]  # Extracts type of each steams (dictionary structure) and assigns it to timeseries_type (e.g., Markers).\n",
    "    timevec = stream['time_stamps']  # Extract the time_stamps (i.e.,LSL TIMESTAMPS) from the \"stream\" dictionary and assigns it to the variable timevec\n",
    "    timeseries = stream['time_series']  # Extract the time_series (i.e., DATA) from the \"stream\" dictionary and\n",
    "    samplerate = round(float(stream['info']['nominal_srate'][0]))  # Extracts the rounded sampling rate (nominal_srate) and assigns it to samplerate\n",
    "    channelcount = int(stream['info']['channel_count'][0])  # Extracts the number of channel for each steams and assigns it to channelcount (as an integer)\n",
    "\n",
    "    # Extract channel labels (this step is a bit more complex because of the XDF file configuration)\n",
    "    channel_labels = []  # Initialize an empty list to store channel labels\n",
    "    # Check if there is more than one channel and if the 'desc' field is present in the stream's info\n",
    "    if channelcount > 1 and stream['info']['desc'] is not None:\n",
    "        try:\n",
    "            channels_info = stream['info']['desc'][0]['channels'][0]['channel']  # Attempt to extract channel information from the stream's description\n",
    "            channel_labels = [channel['label'][0] for channel in channels_info]  # Create a list of channel labels by extracting the 'label' field from each channel's info\n",
    "        except (KeyError, IndexError):\n",
    "            channel_labels = [f\"Channel {i + 1}\" for i in range(channelcount)]  # If there is any issue with extracting channel labels, create default labels\n",
    "    else:\n",
    "        channel_labels = [f\"Channel {i + 1}\" for i in range(channelcount)]  # If there is only one channel or the 'desc' field is not present, create default labels\n",
    "\n",
    "    print('working on stream: ' + timeseries_name + '  with a channel count of: ' + str(channelcount) + ' labelled: ' + str(channel_labels) + ' and a sampling rate of ' + str(samplerate))\n",
    "\n",
    "    # Extracting Relevant Events from Marker Stream that will be used as Selectors in the Plots\n",
    "    if timeseries_type == 'Markers':\n",
    "        audio_events = get_events(stream, event_names)  # Using the get_events function to extract events and corresponding LSL times from Marker stream\n",
    "\n",
    "        # Create selector buttons based on audio events\n",
    "        selector_buttons = []\n",
    "        for i in range(0, len(audio_events), 2):\n",
    "            start_event = audio_events[i]\n",
    "            end_event = audio_events[i + 1]\n",
    "            condition_name = start_event[1].split('_StartParticipantSinging')[0]\n",
    "            selector_buttons.append(dict(label=condition_name,\n",
    "                                         method=\"relayout\",\n",
    "                                         args=[{\"xaxis.range\": [float(start_event[0]), float(end_event[0])]}]))\n",
    "\n",
    "        continue  # Skips plotting for Markers stream\n",
    "\n",
    "    # PLOTTING\n",
    "    timeseries = np.array(timeseries)  # Ensure timeseries is a NumPy array\n",
    "\n",
    "    # Plotting subplots in case of multiple channels\n",
    "    if channelcount > 1:\n",
    "        fig = make_subplots(rows=channelcount, cols=1, shared_xaxes=True, vertical_spacing=0.02, subplot_titles=channel_labels)\n",
    "\n",
    "        for i in range(channelcount):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=timevec,\n",
    "                y=timeseries[:, i],\n",
    "                mode='lines',\n",
    "                name=channel_labels[i]\n",
    "            ), row=i + 1, col=1)\n",
    "\n",
    "        # Update x-axis settings for the last subplot\n",
    "        fig.update_xaxes(\n",
    "            rangeslider=dict(visible=True),\n",
    "            type=\"linear\",\n",
    "            row=channelcount, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title_text=fnam + '_' + timeseries_name + ' Streams',\n",
    "            height=150 * channelcount + 150,\n",
    "            yaxis_title='Amplitude',\n",
    "            updatemenus=[dict(type=\"buttons\", buttons=selector_buttons, direction=\"down\", showactive=True)]\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    else:\n",
    "        # Plotting single channel data stream\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=timevec,\n",
    "            y=timeseries[:, 0],\n",
    "            mode='lines',\n",
    "            name=channel_labels[0]\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=fnam + '_' + timeseries_name + ' Streams',\n",
    "            xaxis=dict(\n",
    "                rangeselector=dict(buttons=selector_buttons),\n",
    "                rangeslider=dict(visible=True),\n",
    "                type=\"linear\"\n",
    "            ),\n",
    "            yaxis_title='Amplitude',\n",
    "            updatemenus=[dict(type=\"buttons\", buttons=selector_buttons, direction=\"down\", showactive=True)]\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14f75b-6907-4213-bd3d-687347aeabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "event_names = ['_StartParticipantSinging', '_EndParticipantSinging']\n",
    "\n",
    "\n",
    "for xdf_file in xdf_files:               # Iterate over each path in the list \"xdf_files\". \n",
    "\n",
    "    print('loading xdf file: ' + xdf_file )\n",
    "\n",
    "    streams, header = pyxdf.load_xdf(xdf_file)    # Loading steams and header information from each XDF file usign the load_xdf function from the pyxdf library. \n",
    "    fnam = os.path.basename(xdf_file)[:-4]        # Extract the file name from the path and assings it to fnam, whilst removing the '.xdf' extension (i.e., the last 4 characters in the string)\n",
    "       \n",
    "    # Renaming streams using the rename_dict and rename_function (if names match dictionary)\n",
    "    streams = rename_streams(streams, rename_dict)\n",
    "\n",
    "    # Navigating through each stream and extracting the relevant information (e.g., name, sample rate, data, time, etc.) \n",
    "    for stream in streams:                        # Iterate over each steam in the loaded steam for each XDF file .\n",
    "        timeseries_name = stream['info']['name'][0]                    # Extracts name of each steams (dictionary structure) and assigns it to timeseries_name (e.g., Mic). \n",
    "        timeseries_type = stream['info']['type'][0]                     # Extracts type of each steams (dictionary structure) and assigns it to timeseries_type (e.g., Markers). \n",
    "        timevec = stream['time_stamps']                                # Extract the time_stamps (i.e.,LSL TIMESTAMPS) from the \"stream\" dictionary and assigns it to the variable timevec\n",
    "        timeseries = stream['time_series']                             # Extract the time_series (i.e., DATA) from the \"stream\" dictionary and \n",
    "        samplerate = round(float(stream['info']['nominal_srate'][0]))  # Extracts the rounded sampling rate (nominal_srate) and assings it to samplerate \n",
    "        channelcount = int(stream['info']['channel_count'][0])         # Extracts the number of channel for each steams and assigns it to channelcount (as an integer)\n",
    "\n",
    "        # Extract channel labels (this step is a bit more complex because of the XDF file configuration)\n",
    "        channel_labels = []  # Initialize an empty list to store channel labels\n",
    "        # Check if there is more than one channel and if the 'desc' field is present in the stream's info\n",
    "        if channelcount > 1 and stream['info']['desc'] is not None:\n",
    "            try:\n",
    "                channels_info = stream['info']['desc'][0]['channels'][0]['channel']  # Attempt to extract channel information from the stream's description\n",
    "                channel_labels = [channel['label'][0] for channel in channels_info]  # Create a list of channel labels by extracting the 'label' field from each channel's info\n",
    "            except (KeyError, IndexError):\n",
    "                channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]  # If there is any issue with extracting channel labels, create default labels\n",
    "        else:\n",
    "            channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]  # If there is only one channel or the 'desc' field is not present, create default labels\n",
    "\n",
    "        \n",
    "        print('working on stream: ' + timeseries_name + '  with a channel count of: ' + str(channelcount) + ' labelled: ' + str(channel_labels) + ' and a sampling rate of ' + str(samplerate)) \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Extracting Relevant Events from Marker Steam that will be used as Selectors in the Plots \n",
    "        if timeseries_type == 'Markers':\n",
    "            audio_events = get_events(stream, event_names)  #Using the get_events function to extracts events and corresponding LSL times from Marker stream\n",
    "\n",
    "             # Create selector buttons based on audio events\n",
    "            selector_buttons = []\n",
    "            for i in range(0, len(audio_events), 2):\n",
    "                start_event = audio_events[i]\n",
    "                end_event = audio_events[i + 1]\n",
    "                condition_name = start_event[1].split('_StartParticipantSinging')[0]\n",
    "                selector_buttons.append(dict(label=condition_name,\n",
    "                                             method=\"relayout\",\n",
    "                                             args=[{\"xaxis.range\": [float(start_event[0]), float(end_event[0])]}]))\n",
    "                        \n",
    "            continue    # Skips plotting for Markers stream \n",
    "            \n",
    "        # PLOTTING \n",
    "        timeseries = np.array(timeseries)   # Ensure timeseries is a NumPy array\n",
    "        \n",
    "         # Plotting subplots in case of multiple channels\n",
    "        if channelcount > 1:\n",
    "            fig = make_subplots(rows=channelcount, cols=1, shared_xaxes=True, vertical_spacing=0.02, subplot_titles=channel_labels)\n",
    "        \n",
    "            for i in range(channelcount):\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=timevec, \n",
    "                    y=timeseries[:, i],\n",
    "                    mode='lines',\n",
    "                    name=channel_labels[i]\n",
    "                ), row=i+1, col=1)\n",
    "        \n",
    "            # Update x-axis settings for the last subplot\n",
    "            fig.update_xaxes(\n",
    "                rangeslider=dict(visible=True),\n",
    "                type=\"linear\",\n",
    "                row=channelcount, col=1\n",
    "            )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title_text=fnam + '_' + timeseries_name + ' Streams',\n",
    "                height=150 * channelcount + 150,\n",
    "                yaxis_title='Amplitude',\n",
    "                updatemenus=[dict(buttons=selector_buttons, direction=\"down\", showactive=True)]\n",
    "            )\n",
    "\n",
    "            fig.show()\n",
    "            \n",
    "        else:\n",
    "            # Plotting single channel data stream\n",
    "            fig = go.Figure()\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=timevec,\n",
    "                y=timeseries[:, 0],\n",
    "                mode='lines',\n",
    "                name=channel_labels[0]\n",
    "            ))\n",
    "\n",
    "            fig.update_layout(\n",
    "                title_text=fnam + '_' + timeseries_name + ' Streams',\n",
    "                xaxis=dict(\n",
    "                    rangeselector=dict(buttons=selector_buttons),\n",
    "                    rangeslider=dict(visible=True),\n",
    "                    type=\"linear\"\n",
    "                ),\n",
    "                yaxis_title='Amplitude',\n",
    "                updatemenus=[dict(buttons=selector_buttons, direction=\"down\", showactive=True)]\n",
    "            )\n",
    "\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b41447-93b3-4604-b800-8e53e73656db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selector_buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80c993-9b8b-4283-b7cf-76b073b33679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "event_names = ['_StartParticipantSinging', '_EndParticipantSinging']\n",
    "\n",
    "\n",
    "for xdf_file in xdf_files:               # Iterate over each path in the list \"xdf_files\". \n",
    "\n",
    "    print('loading xdf file: ' + xdf_file )\n",
    "\n",
    "    streams, header = pyxdf.load_xdf(xdf_file)    # Loading steams and header information from each XDF file usign the load_xdf function from the pyxdf library. \n",
    "    fnam = os.path.basename(xdf_file)[:-4]        # Extract the file name from the path and assings it to fnam, whilst removing the '.xdf' extension (i.e., the last 4 characters in the string)\n",
    "       \n",
    "    # Renaming streams using the rename_dict and rename_function (if names match dictionary)\n",
    "    streams = rename_streams(streams, rename_dict)\n",
    "\n",
    "    # Navigating through each stream and extracting the relevant information (e.g., name, sample rate, data, time, etc.) \n",
    "    for stream in streams:                        # Iterate over each steam in the loaded steam for each XDF file .\n",
    "        timeseries_name = stream['info']['name'][0]                    # Extracts name of each steams (dictionary structure) and assigns it to timeseries_name (e.g., Mic). \n",
    "        timeseries_type = stream['info']['type'][0]                     # Extracts type of each steams (dictionary structure) and assigns it to timeseries_type (e.g., Markers). \n",
    "        timevec = stream['time_stamps']                                # Extract the time_stamps (i.e.,LSL TIMESTAMPS) from the \"stream\" dictionary and assigns it to the variable timevec\n",
    "        timeseries = stream['time_series']                             # Extract the time_series (i.e., DATA) from the \"stream\" dictionary and \n",
    "        samplerate = round(float(stream['info']['nominal_srate'][0]))  # Extracts the rounded sampling rate (nominal_srate) and assings it to samplerate \n",
    "        channelcount = int(stream['info']['channel_count'][0])         # Extracts the number of channel for each steams and assigns it to channelcount (as an integer)\n",
    "\n",
    "        # Extract channel labels (this step is a bit more complex because of the XDF file configuration)\n",
    "        channel_labels = []  # Initialize an empty list to store channel labels\n",
    "        # Check if there is more than one channel and if the 'desc' field is present in the stream's info\n",
    "        if channelcount > 1 and stream['info']['desc'] is not None:\n",
    "            try:\n",
    "                channels_info = stream['info']['desc'][0]['channels'][0]['channel']  # Attempt to extract channel information from the stream's description\n",
    "                channel_labels = [channel['label'][0] for channel in channels_info]  # Create a list of channel labels by extracting the 'label' field from each channel's info\n",
    "            except (KeyError, IndexError):\n",
    "                channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]  # If there is any issue with extracting channel labels, create default labels\n",
    "        else:\n",
    "            channel_labels = [f\"Channel {i+1}\" for i in range(channelcount)]  # If there is only one channel or the 'desc' field is not present, create default labels\n",
    "\n",
    "        \n",
    "        print('working on stream: ' + timeseries_name + '  with a channel count of: ' + str(channelcount) + ' labelled: ' + str(channel_labels) + ' and a sampling rate of ' + str(samplerate)) \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Extracting Relevant Events from Marker Steam that will be used as Selectors in the Plots \n",
    "        if timeseries_type == 'Markers':\n",
    "            audio_events_steam = get_events(stream, event_names)  #Using the get_events function to extracts events and corresponding LSL times from Marker stream\n",
    "\n",
    "            continue    # Skips plotting for Markers stream \n",
    "            \n",
    "        # PLOTTING \n",
    "        timeseries = np.array(timeseries)   # Ensure timeseries is a NumPy array\n",
    "        \n",
    "        # Plotting subplots in case of multiple streams \n",
    "        if channelcount > 1:\n",
    "            fig = make_subplots(rows=channelcount, cols=1, shared_xaxes=True, vertical_spacing=0.02, subplot_titles=channel_labels)\n",
    "        \n",
    "            for i in range(channelcount):\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=timevec, \n",
    "                    y=timeseries[:, i],\n",
    "                    mode='lines',\n",
    "                    name=channel_labels[i]\n",
    "                ), row=i+1, col=1)\n",
    "        \n",
    "                fig.update_yaxes(title_text=channel_labels[i], row=i+1, col=1)\n",
    "        \n",
    "            # Update x-axis settings for all subplots\n",
    "            fig.update_layout(\n",
    "                title_text=fnam + '_' + timeseries_name + ' Streams',\n",
    "                height=300 * channelcount,\n",
    "                xaxis=dict(\n",
    "                    rangeselector=dict(\n",
    "                        buttons=list([\n",
    "                            dict(count=1,\n",
    "                                 label=\"1m\",\n",
    "                                 step=\"minute\",\n",
    "                                 stepmode=\"backward\"),\n",
    "                            dict(count=10,\n",
    "                                 label=\"10m\",\n",
    "                                 step=\"minute\",\n",
    "                                 stepmode=\"backward\"),\n",
    "                            dict(step=\"all\")\n",
    "                        ])\n",
    "                    ),\n",
    "                    rangeslider=dict(\n",
    "                        visible=True\n",
    "                    ),\n",
    "                    type=\"linear\"\n",
    "                ),\n",
    "                yaxis_title='Amplitude'\n",
    "            )\n",
    "        \n",
    "            fig.show()\n",
    "                    \n",
    "        else:\n",
    "            # Plotting single channel data stream\n",
    "            fig = go.Figure()\n",
    "            timeseries = np.array(timeseries)  # Ensure timeseries is a NumPy array\n",
    "        \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=timevec,\n",
    "                y=timeseries[:, 0],\n",
    "                mode='lines',\n",
    "                name=channel_labels[0]\n",
    "            ))\n",
    "        \n",
    "            fig.update_layout(\n",
    "                title_text=fnam + '_' + timeseries_name + ' Streams',\n",
    "                xaxis=dict(\n",
    "                    rangeselector=dict(\n",
    "                        buttons=list([\n",
    "                            dict(count=1,\n",
    "                                 label=\"1m\",\n",
    "                                 step=\"minute\",\n",
    "                                 stepmode=\"backward\"),\n",
    "                            dict(count=10,\n",
    "                                 label=\"10m\",\n",
    "                                 step=\"minute\",\n",
    "                                 stepmode=\"backward\"),\n",
    "                            dict(step=\"all\")\n",
    "                        ])\n",
    "                    ),\n",
    "                    rangeslider=dict(\n",
    "                        visible=True\n",
    "                    ),\n",
    "                    type=\"linear\"\n",
    "                ),\n",
    "                yaxis_title='Amplitude'\n",
    "            )\n",
    "        \n",
    "            fig.show()\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb25f25-6416-42e7-9861-cfbfb11a32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "                     # fig.show()\n",
    "        \n",
    "            #     # Update x-axis settings for the last subplot\n",
    "            #     if i == channelcount-1:\n",
    "            #         fig.update_xaxes(\n",
    "            #             rangeselector=dict(\n",
    "            #                 buttons=list([\n",
    "            #                     dict(count=1,\n",
    "            #                          label=\"1m\",\n",
    "            #                          step=\"minute\",\n",
    "            #                          stepmode=\"backward\"),\n",
    "            #                     dict(count=10,\n",
    "            #                          label=\"10m\",\n",
    "            #                          step=\"minute\",\n",
    "            #                          stepmode=\"backward\"),\n",
    "            #                     dict(step=\"all\")\n",
    "            #                 ])\n",
    "            #             ),\n",
    "            #             rangeslider=dict(\n",
    "            #                 visible=True\n",
    "            #             ),\n",
    "            #             type=\"linear\",\n",
    "            #             row=i+1, col=1\n",
    "                        )\n",
    "\n",
    "            # Plotting Events (i.e., triggers) \n",
    "        # if stream['info']['channel_format'][0] == 'string':\n",
    "        #     plt.figure(figsize=(12, 6))\n",
    "        #     y_pos = np.arange(len(timevec))  # Create a y-position for each event to avoid overlap\n",
    "        #     plt.scatter(timevec, [1] * len(timevec), marker='o')\n",
    "        #     for i, event in enumerate(timeseries):\n",
    "        #         plt.text(timevec[i], 1.01, event[0], rotation=45, ha='right', va='bottom', fontsize=6)\n",
    "        #     plt.title(fnam + '_' + timeseries_type + ' Events')\n",
    "        #     plt.xlabel('Time')\n",
    "        #     plt.yticks([])\n",
    "        #     plt.grid(True)\n",
    "        #     plt.show()\n",
    "\n",
    "\n",
    "        #     else:\n",
    "        #     # Plotting data streams with Plotly subplots if multiple channels\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # # Plotting data streams with subplots and sliders\n",
    "            # fig, axs = plt.subplots(channelcount, 1, figsize=(12, 6 * channelcount), sharex=True)\n",
    "            # timeseries = np.array(timeseries)\n",
    "            # if channelcount == 1:\n",
    "            #     axs = [axs]  # Ensure axs is always a list\n",
    "\n",
    "            # for i in range(channelcount):\n",
    "            #     axs[i].plot(timevec, timeseries[:, i], label=channel_labels[i])\n",
    "            #     axs[i].set_title(f'{channel_labels[i]}')\n",
    "            #     axs[i].set_ylabel('Amplitude')\n",
    "            #     axs[i].grid(True)\n",
    "            #     axs[i].legend()\n",
    "\n",
    "            # axs[-1].set_xlabel('Time')\n",
    "\n",
    "            # # Adding slider for each subplot\n",
    "            # sliders = []\n",
    "            # axcolor = 'lightgoldenrodyellow'\n",
    "            # for i in range(channelcount):\n",
    "            #     ax_slider = plt.axes([0.25, 0.02 + i * 0.04, 0.65, 0.03], facecolor=axcolor)\n",
    "            #     slider = Slider(ax_slider, 'Range', timevec[0], timevec[-1], valinit=timevec[-1])\n",
    "            #     sliders.append(slider)\n",
    "\n",
    "            #     def update(val, ax=axs[i], slider=slider):\n",
    "            #         pos = slider.val\n",
    "            #         ax.set_xlim(timevec[0], pos)\n",
    "            #         fig.canvas.draw_idle()\n",
    "\n",
    "            #     slider.on_changed(update)\n",
    "\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "             \n",
    "        # else:\n",
    "        #         # Plotting data streams \n",
    "        #     plt.figure(figsize=(12, 6))  # Adjust the figure size as needed\n",
    "        #     timeseries = np.array(timeseries)\n",
    "        #     for i in range(channelcount):\n",
    "        #         plt.plot(timevec, timeseries[:, i], label=channel_labels[i])\n",
    "        #     plt.title(fnam + '_' + timeseries_type + ' Streams')\n",
    "        #     plt.xlabel('Time')\n",
    "        #     plt.ylabel('Amplitude')\n",
    "        #     plt.legend()\n",
    "        #     plt.grid(True)\n",
    "        #     plt.show()\n",
    "        ## -------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807487c-cb0c-4b18-acb8-702f0a1e6b21",
   "metadata": {},
   "source": [
    "Add a plotting/quality check passage to see each stream and each channel \n",
    "\n",
    "For Opensignals, maybe add subpanels to change the y axis. \n",
    "    subplots with total height of bigger plot. \n",
    "\n",
    "    Envision box plotly (selecting, smoothing and deriving measures) --> Use the slider range as well \n",
    "\n",
    "Naming the files. \n",
    "    In the lab setup, we will publish the scripts that stream the data to Lab recorder. \n",
    "    Each stream should be named accordingly \n",
    "    For now, we can change the actual stream names in the xdf file manually by using pyXDF (the same names will be used). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49683b68-6380-4a4b-916c-717ee799f219",
   "metadata": {},
   "source": [
    "Creating saving xdf folders for each session. Name of files will be Session_X_P_X_streamname\n",
    "\n",
    "\n",
    "\n",
    "Downsampling the videos \n",
    "\n",
    "go into timevec (original fps is about 200 with LSL)\n",
    "indent every 1/5 and extract the timevec (LSL time) and timeseries data \n",
    "Put into a new matrix\n",
    "Go into the video with this new matrix and "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889186dc-4248-4fa3-9a18-3d61abfc6f10",
   "metadata": {},
   "source": [
    "## 4. Clipping videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829065cc-f797-4d62-a961-705e23d4fa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pilot_01 for pilot_1: pilot_01_d2_speech_lsl_BalanceBoard_stream.csv\n",
      "This is the file path: C:\\Users\\ahmar\\OneDrive\\Documents\\GitHub\\Mobile-Multimodal-Lab\\4_SpeakUp\\data_processed\\pilot_01_d2_speech_lsl_BalanceBoard_stream.csv\n",
      "loading the csv file  pilot_01_d2_speech_lsl_BalanceBoard_stream.csvcontaining LSL times and frames numbers  \n",
      "Downsampling the LSL times and frame numbers to 50fps. This might take some time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling LSL times and frames:  21%|██████████                                      | 5589/26781 [00:10<00:37, 565.41it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m resampled_frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m resampled_time:\n\u001b[1;32m---> 46\u001b[0m     closest_index \u001b[38;5;241m=\u001b[39m (downsampled_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m time)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39midxmin()\n\u001b[0;32m     47\u001b[0m     resampled_frames\u001b[38;5;241m.\u001b[39mappend(downsampled_data\u001b[38;5;241m.\u001b[39miloc[closest_index, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     48\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\series.py:2668\u001b[0m, in \u001b[0;36mSeries.idxmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m   2664\u001b[0m     \u001b[38;5;66;03m# TODO(3.0): this catching/filtering can be removed\u001b[39;00m\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;66;03m# ignore warning produced by argmin since we will issue a different\u001b[39;00m\n\u001b[0;32m   2666\u001b[0m     \u001b[38;5;66;03m#  warning for idxmin\u001b[39;00m\n\u001b[0;32m   2667\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2668\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margmin(axis, skipna, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;66;03m# GH#43587 give correct NA value for Index.\u001b[39;00m\n\u001b[0;32m   2672\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2673\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.idxmin with all-NA \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2674\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, or any-NA and skipna=False, is deprecated. In a future \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   2678\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\base.py:785\u001b[0m, in \u001b[0;36mIndexOpsMixin.argmin\u001b[1;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m delegate\u001b[38;5;241m.\u001b[39margmin()\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 785\u001b[0m     result \u001b[38;5;241m=\u001b[39m nanops\u001b[38;5;241m.\u001b[39mnanargmin(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna)\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    787\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    788\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.argmax/argmin \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith skipna=False and NAs, or with all-NAs is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    792\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    793\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\nanops.py:1193\u001b[0m, in \u001b[0;36mnanargmin\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnanargmin\u001b[39m(\n\u001b[0;32m   1156\u001b[0m     values: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     mask: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m    array([0, 0, 1, 1])\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1193\u001b[0m     values, mask \u001b[38;5;241m=\u001b[39m _get_values(values, \u001b[38;5;28;01mTrue\u001b[39;00m, fill_value_typ\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m   1194\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39margmin(axis)\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_maybe_arg_null_out\" has incompatible type \"Any |\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;66;03m# signedinteger[Any]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\nanops.py:294\u001b[0m, in \u001b[0;36m_get_values\u001b[1;34m(values, skipna, fill_value, fill_value_typ, mask)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03mUtility to get the values view, mask, dtype, dtype_max, and fill_value.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    Mask for values, if deemed necessary to compute\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# In _get_values is only called from within nanops, and in all cases\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m#  with scalar fill_value.  This guarantee is important for the\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m#  np.where call below\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m mask \u001b[38;5;241m=\u001b[39m _maybe_get_mask(values, skipna, mask)\n\u001b[0;32m    296\u001b[0m dtype \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    298\u001b[0m datetimelike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\nanops.py:248\u001b[0m, in \u001b[0;36m_maybe_get_mask\u001b[1;34m(values, skipna, mask)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skipna \u001b[38;5;129;01mor\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 248\u001b[0m         mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\dtypes\\missing.py:178\u001b[0m, in \u001b[0;36misna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_] \u001b[38;5;241m|\u001b[39m NDFrame:\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isna(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\dtypes\\missing.py:207\u001b[0m, in \u001b[0;36m_isna\u001b[1;34m(obj, inf_as_na)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (np\u001b[38;5;241m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isna_array(obj, inf_as_na\u001b[38;5;241m=\u001b[39minf_as_na)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, ABCIndex):\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# Try to use cached isna, which also short-circuits for integer dtypes\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m#  and avoids materializing RangeIndex._values\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_can_hold_na:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\multimodallab\\Lib\\site-packages\\pandas\\core\\dtypes\\missing.py:300\u001b[0m, in \u001b[0;36m_isna_array\u001b[1;34m(values, inf_as_na)\u001b[0m\n\u001b[0;32m    298\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misfinite(values)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m         result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(values)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downsampling LSL times and frames:  21%|██████████                                      | 5623/26781 [00:22<00:37, 565.41it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_video_folder = './video_raw/'    # this folder should only contain the videos you want to process. \n",
    "output_video_folder = './video_cut/'\n",
    "input_file_folder = './data_processed/'\n",
    "\n",
    "# List of participant file pairs\n",
    "participants_files = [\n",
    "    ('pilot_01', 'pilot_1'),\n",
    "]\n",
    "\n",
    "# Loading the relevant CSV files (called here ') that contain the LSL_time stamps and corresponding video frames\n",
    "for participant_frame, participant_video in participants_files:\n",
    "    \n",
    "    # Loading the relevant CSV files for each participant\n",
    "    for file in os.listdir(os.path.abspath(input_file_folder)):\n",
    "        \n",
    "        if participant_frame in file:  # Check for the corresponding participant\n",
    "            \n",
    "            print(f'Processing {participant_frame} for {participant_video}: {file}')\n",
    "            \n",
    "            file_path = os.path.join(os.path.abspath(input_file_folder), file)\n",
    "            print(f'This is the file path: {file_path}')\n",
    "\n",
    "            print('loading the csv file  ' + str(file) + 'containing LSL times and frames numbers  ' ) \n",
    "\n",
    "            # Loading the CSV file\n",
    "            file_data = pd.read_csv(file_path) # Reads the CSV file at the constructed path into a DataFrame called file_data \n",
    "\n",
    "           \n",
    "           #-----------  Downsampling----------------\n",
    "            print('Downsampling the LSL times and frame numbers to 50fps. This might take some time')\n",
    "        \n",
    "            downsampled_data = file_data.copy()    #creating a copy to work with. \n",
    "           \n",
    "            # Resample to 50 fps (every 20ms)\n",
    "            resampled_time = np.arange(downsampled_data.iloc[:, 0].min(), downsampled_data.iloc[:, 0].max(), 1/50)\n",
    "\n",
    "            # Initialize the progress bar\n",
    "            pbar = tqdm(total=len(resampled_time), desc=\"Downsampling LSL times and frames\")\n",
    "\n",
    "\n",
    "            #Looping over the LSL_times to find the closest time and frame index \n",
    "            resampled_frames = []\n",
    "            for time in resampled_time:\n",
    "                closest_index = (downsampled_data.iloc[:, 0] - time).abs().idxmin()\n",
    "                resampled_frames.append(downsampled_data.iloc[closest_index, 1])\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "\n",
    "             # Convert resampled frames to a DataFrame  (useful for further processing)\n",
    "            resampled_data = pd.DataFrame({\n",
    "                'LSL_time': resampled_time,\n",
    "                'Video_Frames': resampled_frames\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "            # Loading the original videos corresponding to the CSV files that will be cut according to resampled_data\n",
    "            for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "                \n",
    "                if participant_video in video:  # Check for the corresponding participant  \n",
    "                    \n",
    "                    print('Now loading video: ' + video)\n",
    "                    \n",
    "                    video_filepath = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "                    capture = cv2.VideoCapture(video_filepath)  \n",
    "                \n",
    "                    # Extracting relevant meta-data about the video using CV2\n",
    "                    print('Now extracting metadata from video: ' + video)\n",
    "                    \n",
    "                    video_frame_width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))  \n",
    "                    video_frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))  \n",
    "                    video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n",
    "                    video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "                    \n",
    "                    print('video_tot_frames: ' +  str(video_tot_frames))\n",
    "                    print('Video frames per second: ' + str(video_frame_rate))\n",
    "            \n",
    "            \n",
    "                     # ------------  Downsampling Videos -----------------\n",
    "                    print('Now re-writing the video ' + video + ' based on the donwsampled LSL times and frames')\n",
    "            \n",
    "                    # Prepare to write the new video\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the AVI format. 'XVID' is commonly used for AVI files.\n",
    "                    output_filepath = os.path.join(output_video_folder, f'cut_{participant_video}.avi')  # Set the path for the output video file with an AVI extension.\n",
    "                    out = cv2.VideoWriter(output_filepath, fourcc, 50, (video_frame_width, video_frame_height))  # Initialize the VideoWriter object to write frames to a new video file.\n",
    "\n",
    "                    # Read and write the resampled frames\n",
    "                    frame_count = 0  # Initialize a counter for the current frame being processed\n",
    "                    with tqdm(total=video_tot_frames, desc=\"Rewriting Video Progress\", leave=False, ncols=100) as pbar:\n",
    "                        while capture.isOpened():\n",
    "                            # Read the next frame\n",
    "                            ret, frame = capture.read()\n",
    "                            if ret:\n",
    "                                # Increment the frame count\n",
    "                                frame_count += 1 \n",
    "                                pbar.update(1)  # Update the progress bar\n",
    "                                \n",
    "                                if frame_count in resampled_frames:\n",
    "                                    #cv2.imshow('frame',frame)\n",
    "                                    #cv2.waitKey(1000/30)\n",
    "                                    out.write(frame)\n",
    "                                pbar.update(1)  # Update the progress bar\n",
    "                            else:\n",
    "                                break\n",
    "            \n",
    "                    \n",
    "                    capture.release()\n",
    "                    out.release()\n",
    "                    \n",
    "                    print(f'Video saved as {output_filepath}')\n",
    "            \n",
    "print(\"Done with cutting all videos! You can now look into your folder: \" + output_video_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0f6d3-786d-4352-8559-636ef74d8b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413dc03-1363-4302-a4a6-070c9f3b5e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3895f6-9079-4af8-b888-22043ea4b63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325db20-274e-4570-86d0-e2551f844a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting relevant information from this CSV file \n",
    "LSL_begin_time = resampled_data.iloc[:,0].min()       # Extracts the minimum value from the first column in the resampled_data (i.e., the first LSL_timestamps). \n",
    "LSL_begin_frame = int(resampled_data.iloc[:,1].min())  # Extracts the minimum value from the second column in the resampled_data (i.e., the first video frame)\n",
    "LSL_end_time = resampled_data.iloc[:,0].max()          # Extracts the maximum value from the first column in the resampled_data (i.e., the last LSL_timestamps).\n",
    "LSL_end_frame = int(resampled_data.iloc[:,1].max())    # Extracts the maximum value from the second column in the file_data (i.e., the last video frame).\n",
    "\n",
    "LSL_tot_frames = LSL_end_frame - LSL_begin_frame    # Total number of frames from start to finish recording.\n",
    "LSL_frames = range(LSL_begin_frame, LSL_end_frame)  # Sequence of all frames numbered from the start to the end. \n",
    "\n",
    "LSL_fps = round((LSL_tot_frames / (LSL_end_time - LSL_begin_time)), 3)\n",
    "\n",
    "print(len(resampled_time))\n",
    "print(len(file_data))\n",
    "\n",
    "print(LSL_begin_time)\n",
    "print(LSL_begin_frame)\n",
    "print(LSL_end_frame)\n",
    "print(LSL_end_frame)\n",
    "print(LSL_fps)\n",
    "\n",
    "print(resampled_time)\n",
    "# print(resampled_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0d266-ee15-4a21-b475-7c1011114393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_video_folder = './video_raw/'    # this folder should only contain the videos you want to process. \n",
    "output_video_folder = './video_cut_2/'\n",
    "input_file_folder = './data_processed/'\n",
    "\n",
    "# List of participant file pairs\n",
    "participants_files = [\n",
    "    ('Video_P1', 'P1'),\n",
    "    ('Video_P2', 'P2')\n",
    "]\n",
    "\n",
    "\n",
    "# Loading the original videos corresponding to the CSV files that will be cut according to resampled_data\n",
    "for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "    \n",
    "    if participant_video in video:  # Check for the corresponding participant  \n",
    "        \n",
    "        print('Now loading video: ' + video)\n",
    "        \n",
    "        video_filepath = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "        capture = cv2.VideoCapture(video_filepath)  \n",
    "    \n",
    "        # Extracting relevant meta-data about the video using CV2\n",
    "        print('Now extracting metadata from video: ' + video)\n",
    "        \n",
    "        video_frame_width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))  \n",
    "        video_frame_height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))  \n",
    "        video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n",
    "        video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "        \n",
    "        print('video_tot_frames: ' +  str(video_tot_frames))\n",
    "        print('Video frames per second: ' + str(video_frame_rate))\n",
    "\n",
    "\n",
    "         # ------------  Downsampling Videos -----------------\n",
    "        print('Now re-writing the video ' + video + ' based on the donwsampled LSL times and frames')\n",
    "\n",
    "        # Prepare to write the new video\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the AVI format. 'XVID' is commonly used for AVI files.\n",
    "        output_filepath = os.path.join(output_video_folder, f'cut_{participant_video}.avi')  # Set the path for the output video file with an AVI extension.\n",
    "        out = cv2.VideoWriter(output_filepath, fourcc, 50, (video_frame_width, video_frame_height))  # Initialize the VideoWriter object to write frames to a new video file.\n",
    "        \n",
    "        # Read and write the resampled frames\n",
    "        frame_count = 0  # Initialize a counter for the current frame being processed.\n",
    "        with tqdm(total=video_tot_frames, desc=\"Processing Video Frames\") as pbar:\n",
    "            while capture.isOpened():\n",
    "                # Read the next frame\n",
    "                ret, frame = capture.read()\n",
    "                if ret:\n",
    "                    # Increment the frame count\n",
    "                    frame_count += 1 \n",
    "                    if frame_count in resampled_frames:\n",
    "                        #cv2.imshow('frame',frame)\n",
    "                        #cv2.waitKey(1000/30)\n",
    "                        out.write(frame)\n",
    "                    pbar.update(1)  # Update the progress bar\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        \n",
    "        capture.release()\n",
    "        out.release()\n",
    "        print(f'Video saved as {output_filepath}')\n",
    "\n",
    "print(\"Done with cutting all videos! You can now look into your folder: \" + output_video_folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        #     if frame_count > video_tot_frames:\n",
    "        #         capture.release()\n",
    "        #         out.release() \n",
    "        \n",
    "        # capture.release()\n",
    "        # out.release()\n",
    "        # print(f'Video saved as {output_filepath}')\n",
    "\n",
    "        # # Prepare to write the new video\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the AVI format. 'XVID' is commonly used for AVI files.\n",
    "        # output_filepath = os.path.join(output_video_folder, f'cut_{participant_video}.avi')  # Set the path for the output video file with an AVI extension.\n",
    "        # out = cv2.VideoWriter(output_filepath, fourcc, 50, (video_frame_width, video_frame_height))  # Initialize the VideoWriter object to write frames to a new video file.\n",
    "        \n",
    "        # # Read and write the resampled frames\n",
    "        # current_frame = 0  # Initialize a counter for the current frame being processed.\n",
    "        # for frame_idx in resampled_frames:  # Loop through each frame index in the resampled frames list.\n",
    "        #     capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)  # Set the video capture to the position of the desired frame index.\n",
    "        #     ret, frame = capture.read()  # Read the frame at the current position.\n",
    "        #     if not ret:  # If the frame is not read correctly (e.g., end of the video), break the loop.\n",
    "        #         print(f'Frame at index {frame_idx} could not be read.')\n",
    "        #         break\n",
    "        #     out.write(frame)  # Write the read frame to the new video file.\n",
    "        #     current_frame += 1  # Increment the frame counter.          \n",
    "\n",
    "        #  while capture.isOpened():\n",
    "        #                 # Read the next frame\n",
    "        #     ret, frame = capture.read()\n",
    "        #     if ret:\n",
    "        #          # Increment the frame count\n",
    "        #         frame_count += 1 \n",
    "        #         if frame_count in resampled_frames:\n",
    "        #             #cv2.imshow('frame',frame)\n",
    "        #             #cv2.waitKey(1000/30)\n",
    "        #             out.write(frame)\n",
    "        #     if frame > video_tot_frames:\n",
    "        #         capture.release()\n",
    "        #         out.release() \n",
    "        \n",
    "        # capture.release()\n",
    "        # out.release()\n",
    "        # print(f'Video saved as {output_filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ba44d-d1f8-45de-bc19-458541bcb84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975c794-9acf-4f6d-9199-5f5a9556a763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd6f26-276c-4834-8082-8ebd9e160e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247571c7-2592-4a94-ba32-871fa53cecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "             # Loading the original videos corresponding to the CSV files that will be cut according to resampled_data\n",
    "            for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "                \n",
    "                if participant_video in video:  # Check for the corresponding participant  \n",
    "                    \n",
    "                    print('Now loading video: ' + video)\n",
    "                    \n",
    "                    video_filepath = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "                    capture = cv2.VideoCapture(video_filepath)  \n",
    "                \n",
    "                    # Extracting relevant meta-data about the video using CV2\n",
    "                    print('Now extracting metadata from video: ' + video)\n",
    "                    \n",
    "                    video_frame_width  = capture.get(cv2.CAP_PROP_FRAME_WIDTH)  \n",
    "                    video_frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)  \n",
    "                    video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n",
    "                    video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "                    capture.release()\n",
    "                    print('video_tot_frames: ' +  str(video_tot_frames))\n",
    "                    print('Video frames per second: ' + str(video_frame_rate))\n",
    "\n",
    "\n",
    "                     # ------------  Downsampling Videos -----------------\n",
    "                    print('Now re-writing the video ' + video + ' based on the donwsampled LSL times and frames')\n",
    "\n",
    "                    # Prepare to write the new video\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Specify the codec for the AVI format. 'XVID' is commonly used for AVI files.\n",
    "                    output_filepath = os.path.join(output_video_folder, f'cut_{participant_video}.avi')  # Set the path for the output video file with an AVI extension.\n",
    "                    out = cv2.VideoWriter(output_filepath, fourcc, 50, (video_frame_width, video_frame_height))  # Initialize the VideoWriter object to write frames to a new video file.\n",
    "                    \n",
    "                    # Read and write the resampled frames\n",
    "                    current_frame = 0  # Initialize a counter for the current frame being processed.\n",
    "                    for frame_idx in resampled_frames:  # Loop through each frame index in the resampled frames list.\n",
    "                        capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)  # Set the video capture to the position of the desired frame index.\n",
    "                        ret, frame = capture.read()  # Read the frame at the current position.\n",
    "                        if not ret:  # If the frame is not read correctly (e.g., end of the video), break the loop.\n",
    "                            break\n",
    "                        out.write(frame)  # Write the read frame to the new video file.\n",
    "                        current_frame += 1  # Increment the frame counter.\n",
    "\n",
    "                    capture.release()\n",
    "                    out.release()\n",
    "                    print(f'Video saved as {output_filepath}')\n",
    "\n",
    "            \n",
    "\n",
    "                          \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            # Extracting relevant information from this resampled_data\n",
    "            LSL_begin_time = resampled_data.iloc[:,0].min()       # Extracts the minimum value from the first column in the resampled_data (i.e., the first LSL_timestamps). \n",
    "            LSL_begin_frame = int(resampled_data.iloc[:,1].min())  # Extracts the minimum value from the second column in the resampled_data (i.e., the first video frame)\n",
    "            LSL_end_time = resampled_data.iloc[:,0].max()          # Extracts the maximum value from the first column in the resampled_data (i.e., the last LSL_timestamps).\n",
    "            LSL_end_frame = int(resampled_data.iloc[:,1].max())    # Extracts the maximum value from the second column in the file_data (i.e., the last video frame).\n",
    "\n",
    "            LSL_tot_frames = LSL_end_frame - LSL_begin_frame    # Total number of frames from start to finish recording.\n",
    "            LSL_frames = range(LSL_begin_frame, LSL_end_frame)  # Sequence of all frames numbered from the start to the end. \n",
    "            \n",
    "            LSL_fps = round((LSL_tot_frames / (LSL_end_time - LSL_begin_time)), 3)\n",
    "                           \n",
    "\n",
    "            # Loading the original videos corresponding to the CSV files that will be cut according to LSL start and ends\n",
    "            for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "                \n",
    "                if participant_video in video:  # Check for the corresponding participant  \n",
    "                    \n",
    "                    print('Now processing video: ' + video)\n",
    "                    \n",
    "                    video_filepath = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "                    capture = cv2.VideoCapture(video_filepath)  \n",
    "                \n",
    "                    # Extracting relevant meta-data about the video using CV2\n",
    "                    video_frame_width  = capture.get(cv2.CAP_PROP_FRAME_WIDTH)  \n",
    "                    video_frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)  \n",
    "                    video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n",
    "                    video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "                    capture.release()\n",
    "                    print('video_tot_frames: ' +  str(video_tot_frames))\n",
    "                    print('Video frames per second: ' + str(video_frame_rate))\n",
    "        \n",
    "                    # Cutting video Using ffmpeg \n",
    "                    # Converting the LSL frames to the video time format to find start_cut and end_cut for the video\n",
    "                    start_cut_time = frame_to_time(LSL_begin_frame, video_frame_rate)\n",
    "                    end_cut_time = frame_to_time(LSL_end_frame, video_frame_rate)\n",
    "        \n",
    "                    print('Now cutting the video...')\n",
    "        \n",
    "                    # Determine the file extension and codec\n",
    "                    file_extension = os.path.splitext(video)[1].lower()\n",
    "                    codec = extension_to_codec.get(file_extension, 'libx264')  # Default to libx264 if not found\n",
    "        \n",
    "                    # Construct output file path with the same extension\n",
    "                    output_filename = f'cut_{os.path.splitext(video)[0]}{file_extension}'\n",
    "                    output_filepath = os.path.join(output_video_folder, output_filename)\n",
    "        \n",
    "                    # Use ffmpeg to cut the video\n",
    "                    ffmpeg_command = [\n",
    "                        'ffmpeg',\n",
    "                        '-y',                  # Add -y flag to overwrite any existing files with the same name\n",
    "                        '-i', video_filepath,\n",
    "                        '-ss', start_cut_time,  # start time\n",
    "                        '-to', end_cut_time,    # end time\n",
    "                        '-c', 'copy',           # copy codec (no re-encoding)\n",
    "                        output_filepath]\n",
    "                    \n",
    "                    # Execute the command\n",
    "                    subprocess.run(ffmpeg_command, check=True)\n",
    "        \n",
    "                    print(f'Video saved as {output_filepath}')\n",
    "            \n",
    "\n",
    "print(\"Done with cutting all videos! You can now look into your folder: \" + output_video_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3738df-2cb7-40ff-99cb-e0b7b4050bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3904c-20f2-46ac-8d2d-3421f85af4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fea48c-e33e-4f0a-988a-a13f06b25d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_folder = './video_raw/'    #this folder should only contain the videos you want to process. \n",
    "output_video_folder = './video_cut/'\n",
    "input_file_folder = './data_processed/'\n",
    "\n",
    "# List of participant file pairs       #--------------- SOULD WE DO THIS ANOTHER WAY? MAYBE SAVING THE VSC FILES WITH A CONSISTENT NAME? ---- \n",
    "participants_files = [\n",
    "    ('Video_P1', 'P1'),\n",
    "    ('Video_P2', 'P2')\n",
    "]\n",
    "\n",
    "\n",
    "# Loading the relevant CVS files (called here ') that contain the LSL_time stamps and correspnding video frames\n",
    "for participant_frame, participant_video in participants_files:\n",
    "    \n",
    "    # Loading the relevant CSV files for each participant\n",
    "    for file in os.listdir(os.path.abspath(input_file_folder)):\n",
    "        \n",
    "        if participant_frame in file:  # Check for the corresponding participant\n",
    "            \n",
    "            print(f'Processing {participant_frame} for {participant_video}: {file}')\n",
    "            \n",
    "            file_path = os.path.join(os.path.abspath(input_file_folder), file)\n",
    "            print(f'This is the file path: {file_path}')\n",
    "\n",
    "            # Loading the CSV file\n",
    "            file_data = pd.read_csv(file_path) # Reads the CSV file at the constructed path into a DataFrame called file_data        \n",
    "\n",
    "            # Resample to 50 fps (every 20ms)\n",
    "            resampled_time = pd.date_range(start=file_data.iloc[:, 0].min(), end=file_data.iloc[:, 0].max(), freq='20ms')\n",
    "\n",
    "            resampled_frames = []\n",
    "            for time in resampled_time:\n",
    "                closest_index = (file_data.iloc[:, 0] - time).abs().idxmin()\n",
    "                resampled_frames.append(file_data.iloc[closest_index, 1])\n",
    "\n",
    "            # Extracting relevant information from this CSV file \n",
    "            LSL_begin_time = file_data.iloc[:,0].min()       # Extracts the minimum value from the first column in the file_data (i.e., the first LSL_timestamps). \n",
    "            LSL_begin_frame = int(file_data.iloc[:,1].min())  # Extracts the minimum value from the second column in the file_data (i.e., the first video frame)\n",
    "            LSL_end_time = file_data.iloc[:,0].max()          # Extracts the maximum value from the first column in the file_data (i.e., the last LSL_timestamps).\n",
    "            LSL_end_frame = int(file_data.iloc[:,1].max())    # Extracts the maximum value from the second column in the file_data (i.e., the last video frame).\n",
    "\n",
    "            # print(LSL_begin_time )\n",
    "            # print(LSL_begin_frame)\n",
    "            # print(LSL_end_time)\n",
    "            # print(LSL_end_frame)\n",
    "\n",
    "            \n",
    "\n",
    "            LSL_tot_frames = LSL_end_frame - LSL_begin_frame    # Total number of frames from start to finish recording.\n",
    "            LSL_frames = range(LSL_begin_frame , LSL_end_frame)  # Sequence of all frames numbered from the start to the end. \n",
    "            'Add comment about Theoretical understanding of frame numbers and LSL fps streaming' \n",
    "            \n",
    "            LSL_fps = round((LSL_tot_frames / (LSL_end_time - LSL_begin_time)), 3)\n",
    "\n",
    "            0/0\n",
    "            \n",
    "            new_range = (LSL_begin_time : 1/50 : LSL_end_time) \n",
    "            print(new_range) \n",
    "\n",
    "            framelist = []\n",
    "            for i in new_range: \n",
    "                index_time = which.min(file_data.iloc[:,0]%%i)\n",
    "                index_frame = file_data.iloc[index_time:1]\n",
    "                framelist.append(index_frame) \n",
    "                \n",
    "            print('LSL_tot_frames: ' + str(LSL_tot_frames))\n",
    "            print('LSL frames per second: ' + str(LSL_fps))\n",
    "            'Is this LSL_fps variable??' \n",
    "\n",
    "\n",
    "            0/0\n",
    "            # ----------------------------------------\n",
    "            # Loading the original videos corresponding to the CSV files that will be cut according to LSL start and ends\n",
    "            for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "                \n",
    "                if participant_video in video:  # Check for the corresponding participant  \n",
    "                    \n",
    "                    print('Now processing video: ' + video)\n",
    "                    \n",
    "                    video_filepath = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "                    capture = cv2.VideoCapture(video_filepath)  \n",
    "                \n",
    "                    # Extracting relevant meta-data about the video using CV2\n",
    "                    video_frame_width  = capture.get(cv2.CAP_PROP_FRAME_WIDTH)  \n",
    "                    video_frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)  \n",
    "                    video_frame_rate   = capture.get(cv2.CAP_PROP_FPS)\n",
    "                    video_tot_frames   = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "                    capture.release()\n",
    "                    print('video_tot_frames: ' +  str(video_tot_frames))\n",
    "                    print('Video frames per second: ' + str(video_frame_rate))\n",
    "        \n",
    "        \n",
    "                    ## ---------- Cutting video Using ffmpeg \n",
    "                    # Converting the LSL frames to the video time format to find start_cut and end_cut for the video\n",
    "                    start_cut_time = frame_to_time(LSL_begin_frame, video_frame_rate)\n",
    "                    end_cut_time = frame_to_time(LSL_end_frame, video_frame_rate)\n",
    "        \n",
    "                    print('Now cutting the video...')\n",
    "        \n",
    "                    # Determine the file extension and codec\n",
    "                    file_extension = os.path.splitext(video)[1].lower()\n",
    "                    codec = extension_to_codec.get(file_extension, 'libx264')  # Default to libx264 if not found\n",
    "        \n",
    "                    # Construct output file path with the same extension\n",
    "                    output_filename = f'cut_{os.path.splitext(video)[0]}{file_extension}'\n",
    "                    output_filepath = os.path.join(output_video_folder, output_filename)\n",
    "        \n",
    "                    # Use ffmpeg to cut the video\n",
    "                    ffmpeg_command = [\n",
    "                        'ffmpeg',\n",
    "                        '-y',                  # Add -y flag to overwrite any existing files with the same name\n",
    "                        '-i', video_filepath,\n",
    "                        '-ss', start_cut_time,  # start time\n",
    "                        '-to', end_cut_time,    # end time\n",
    "                        '-c', 'copy',           # copy codec (no re-encoding)\n",
    "                        output_filepath]\n",
    "                    \n",
    "        \n",
    "                    # Execute the command\n",
    "                    subprocess.run(ffmpeg_command, check=True)\n",
    "        \n",
    "                    print(f'Video saved as {output_filepath}')\n",
    "            \n",
    "\n",
    "print(\"Done with cutting all videos! You can now look into your folder: \" + output_video_folder)\n",
    "\n",
    "\n",
    "\n",
    "            ## -------------- Usign moviepy does not work!! Why??\n",
    "            \n",
    "            # # Extract video frame rate using moviepy\n",
    "            # video_clip = VideoFileClip(video_filepath)\n",
    "            # video_frame_rate = video_clip.fps\n",
    "            # print('Video frames per second: ' + str(video_frame_rate))\n",
    "            \n",
    "\n",
    "            # # Converting the LSL frames to the video time format to find start_cut and end_cut for the video \n",
    "            # start_cut_time = LSL_begin_frame / video_frame_rate\n",
    "            # end_cut_time   = LSL_end_frame / video_frame_rate\n",
    "\n",
    "            # print('Now cutting the video...') \n",
    "\n",
    "            # # Determine the file extension and codec\n",
    "            # file_extension = os.path.splitext(video)[1].lower()\n",
    "            # codec = extension_to_codec.get(file_extension)\n",
    "\n",
    "            # if codec is None:\n",
    "            #     print(f\"Unsupported file extension: {file_extension}. Skipping file.\")\n",
    "            #     continue\n",
    "\n",
    "            # # Cut and save the video using MoviePy\n",
    "            # cut_clip = video_clip.subclip(start_cut_time, end_cut_time)\n",
    "\n",
    "            # # Construct output file path with the same extension\n",
    "            # output_filename = f'cut_{os.path.splitext(video)[0]}{file_extension}'\n",
    "            # output_filepath = os.path.join(output_video_folder, output_filename)\n",
    "\n",
    "            # # Save the cut video with the determined codec in specified location\n",
    "            # cut_clip.write_videofile(output_filepath, codec=codec)\n",
    "\n",
    "            # print(f'Video saved as {output_filepath}')\n",
    "\n",
    "\n",
    "            # # -------------- This other way does not work either \n",
    "\n",
    "        #     # Translating the start and end time points to cut the video by multiplying the duration of each video frame by the \n",
    "        #     video_start_frametime_LSL =  np.round((1/int(frate)) * int(begin_frame), 3)\n",
    "        #     video_end_frametime_LSL = np.round((1/int(frate)) * int(end_frame), 3)\n",
    "\n",
    "\n",
    "        #     video_cut = VideoFileClip(video_filepath).cutout(video_start_frametime, video_end_frametime)\n",
    "        #     video_cut.write_videofile(videofolder + \"test.mp4\")\n",
    "\n",
    "\n",
    "\n",
    "        ## ---------------- This way takes a very long time \n",
    "\n",
    "        #   # Start Writing the Video \n",
    "            # fourcc = cv2.VideoWriter_fourcc(*'M', 'J', 'P', 'G')  # For different video formats you could use e.g., *'XVID'\n",
    "            # vidloc = os.path.join(videofolder, f'{video.split(\".\")[0]}_cut.mp4')  # Location to save the new video\n",
    "            # out = cv2.VideoWriter(vidloc, fourcc, fps=originalfps, frameSize=(int(frameWidth), int(frameHeight)))\n",
    "            # frame_count = 0\n",
    "\n",
    "            # print('Looping over the frames')\n",
    "        \n",
    "            # while capture.isOpened():\n",
    "            #     # Read the next frame\n",
    "            #     ret, frame = capture.read()\n",
    "            #     if ret:\n",
    "            #         # Increment the frame count\n",
    "            #         frame_count += 1\n",
    "            #         print(frame_count)\n",
    "            #         if frame_count in frames:\n",
    "            #             out.write(frame)\n",
    "            #         if frame_count > end_frame:\n",
    "            #             break\n",
    "\n",
    "            # capture.release()\n",
    "            # out.release()\n",
    "            # print(f'Video saved to {vidloc}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd35add-78d9-40b2-b773-632b8732e7ba",
   "metadata": {},
   "source": [
    "# 5. Concatenate (cut) Videos with Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb1dc3-f58a-457b-9895-ef414b25150f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aab742c-a79d-4233-89a2-357972edb3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating in the input audio folder: \n",
      "T1_experiment_AudioEvents.csv\n",
      "T1_experiment_Mic_P1.csv\n",
      "T1_experiment_Mic_P1.wav\n",
      "T1_experiment_Mic_P1_denoised.wav\n",
      "Now processing audio T1_experiment_Mic_P1_denoised.wav\n",
      "Loading the audio \n",
      "F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\data_processed\\T1_experiment_Mic_P1_denoised.wav\n",
      "Navigating in the input video folder: \n",
      "cut_P1.avi\n",
      "Now processing video file cut_P1.avi\n",
      "Loading the video \n",
      "F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\video_cut\\cut_P1.avi\n",
      "F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\audiovideo_sync\\P1_audiovideo_sync.avi\n",
      "Combining Audio and Video\n",
      "\n",
      "Video saved as F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\audiovideo_sync\\P1_audiovideo_sync.avi\n",
      "cut_P2.avi\n",
      "T1_experiment_Mic_P2.csv\n",
      "T1_experiment_Mic_P2.wav\n",
      "T1_experiment_Mic_P2_denoised.wav\n",
      "T1_experiment_PLUX_P1.csv\n",
      "T1_experiment_PLUX_P2.csv\n",
      "T1_experiment_Video_P1.csv\n",
      "T1_experiment_Video_P2.csv\n",
      "Navigating in the input audio folder: \n",
      "T1_experiment_AudioEvents.csv\n",
      "T1_experiment_Mic_P1.csv\n",
      "T1_experiment_Mic_P1.wav\n",
      "T1_experiment_Mic_P1_denoised.wav\n",
      "T1_experiment_Mic_P2.csv\n",
      "T1_experiment_Mic_P2.wav\n",
      "T1_experiment_Mic_P2_denoised.wav\n",
      "Now processing audio T1_experiment_Mic_P2_denoised.wav\n",
      "Loading the audio \n",
      "F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\data_processed\\T1_experiment_Mic_P2_denoised.wav\n",
      "Navigating in the input video folder: \n",
      "cut_P1.avi\n",
      "cut_P2.avi\n",
      "Now processing video file cut_P2.avi\n",
      "Loading the video \n",
      "F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\video_cut\\cut_P2.avi\n",
      "F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\audiovideo_sync\\P2_audiovideo_sync.avi\n",
      "Combining Audio and Video\n",
      "\n",
      "Video saved as F:\\Mobile-Multimodal-Lab\\2_PREPROCESSING\\XDF_PROCESSING\\audiovideo_sync\\P2_audiovideo_sync.avi\n",
      "T1_experiment_PLUX_P1.csv\n",
      "T1_experiment_PLUX_P2.csv\n",
      "T1_experiment_Video_P1.csv\n",
      "T1_experiment_Video_P2.csv\n",
      "Done, you can now look into the folder. ./audiovideo_sync/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_audio_folder = './data_processed/' \n",
    "input_video_folder = './video_cut/'\n",
    "output_audiovideo = './audiovideo_sync/'\n",
    "\n",
    "# List of participant file pairs \n",
    "participants_files = [\n",
    "    ('Mic_P1_denoised', 'P1'),\n",
    "    ('Mic_P2_denoised', 'P2')\n",
    "]\n",
    "\n",
    "\n",
    "# loop over Audio files\n",
    "for participant_audio, participant_video in participants_files:\n",
    "    \n",
    "    print('Navigating in the input audio folder: ')\n",
    "    \n",
    "    for audio in os.listdir(os.path.abspath(input_audio_folder)):\n",
    "        print(audio)\n",
    "        \n",
    "        if participant_audio in audio:    #Participant Check \n",
    "            print('Now processing audio '+ audio)\n",
    "        \n",
    "            # Creating audio path \n",
    "            print('Loading the audio ')\n",
    "            audio_path = os.path.join(os.path.abspath(input_audio_folder), audio)\n",
    "            print(audio_path)\n",
    "        \n",
    "        \n",
    "            # Loop over video files to select relevant video \n",
    "            print('Navigating in the input video folder: ')\n",
    "            for video in os.listdir(os.path.abspath(input_video_folder)):\n",
    "                print(video) \n",
    "                \n",
    "                if participant_video in video:\n",
    "                    print('Now processing video file ' + video) \n",
    "        \n",
    "                    # Creating video path \n",
    "                    print('Loading the video ')\n",
    "                    video_path = os.path.join(os.path.abspath(input_video_folder), video)\n",
    "                    print(video_path)\n",
    "        \n",
    "        \n",
    "                    # --- Combining Audio and Video using ffmpeg \n",
    "                    output_path = os.path.abspath(os.path.join(output_audiovideo + str(participant_video) + '_audiovideo_sync.avi'))\n",
    "                    print(output_path)\n",
    "        \n",
    "                    # Construct the ffmpeg command\n",
    "                    ffmpeg_command = [\n",
    "                        'ffmpeg',\n",
    "                        '-y',             #override\n",
    "                        '-i', video_path,\n",
    "                        '-i', audio_path,\n",
    "                        '-c:v', 'copy',  # Copy the video codec\n",
    "                        '-c:a', 'aac',   # Encode audio to AAC\n",
    "                        '-strict', 'experimental',\n",
    "                        output_path\n",
    "                                    ]     \n",
    "                    \n",
    "                    # Run the ffmpeg command\n",
    "                    print('Combining Audio and Video')\n",
    "                    try:\n",
    "                        result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "                        print(result.stdout)\n",
    "                        print(f'Video saved as {output_path}')\n",
    "                    except subprocess.CalledProcessError as e:\n",
    "                        print(f\"Error combining audio and video {video_path} and {audio_path}: {e.stderr}\")\n",
    "                    \n",
    "print('Done, you can now look into the folder. ' + output_audiovideo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003a551-6ecd-4baf-8c29-9a96d77761f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_path = os.path.join(os.path.abspath(input_audio_folder), file)\n",
    "                print(audio_path)\n",
    "        \n",
    "                \n",
    "                if not os.path.exists(audio_path):\n",
    "                    print(f\"Audio file not found: {audio_path}\" + '/n please check your foler to make sure the audio is there')\n",
    "                    \n",
    "                # input the video with ffmpg\n",
    "                input_audio = ffmpeg.input(audio_path)\n",
    "                print(input_audio)\n",
    "        \n",
    "                0/0\n",
    "\n",
    "#load in the video with matchich trialIndex and SessionIndex\n",
    "        print('Loading the video')\n",
    "        video_path = os.path.join(trialfolder, f\"{sessionIndex}_trial_{trialIndex}_video_raw.mp4\")\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video file not found: {video_path}\")\n",
    "        input_video = ffmpeg.input(video_path)\n",
    "        print(input_video)\n",
    "        # get information about the vid_frate\n",
    "        #streamloc = trialfolder+sessionIndex+'_trial_'+ str(trialIndex) +'_'+'MyWebcamFrameStream_nominal_srate500'+'.csv'\n",
    "        #print(streamloc)\n",
    "        #streamdata = pd.read_csv(streamloc)\n",
    "        # get the begin and end frame\n",
    "        #begfr = streamdata['1'].min().astype(int)\n",
    "        #print(begfr)\n",
    "        #endfr = streamdata['1'].max().astype(int)\n",
    "        #print(endfr)\n",
    "        #totfr = endfr-begfr\n",
    "        #print(totfr)\n",
    "        #begin = streamdata['0'].min()\n",
    "        #print(begin)\n",
    "        #end = streamdata['0'].max()\n",
    "        #print(end)\n",
    "        # what is the original fps of the video\n",
    "        #origfps = round((totfr/(end-begin)),3)\n",
    "        # tranform it into real number\n",
    "        #origfps = float(origfps)\n",
    "        \n",
    "        #combine the audio and video\n",
    "        print('Combining audio and video')\n",
    "        output_path = os.path.abspath(os.path.join(trialfolder, f\"{sessionIndex}_trial_{trialIndex}_final.mp4\"))\n",
    "        ffmpeg.concat(input_video, input_audio, v=1, a=1).output(output_path).run(overwrite_output=True)\n",
    "        \n",
    "        #save it\n",
    "        print('Saving the video')\n",
    "        #print(origfps)\n",
    "        #print(type(origfps))\n",
    "        #print(trialIndex)\n",
    "        #print(sessionIndex)\n",
    "        # save the final video with audio\n",
    "        #final.write_videofile(trialfolder+sessionIndex+'_trial_'+ str(trialIndex) +'_'+'video_audio'+'.mp4', fps=origfps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705c5c7-5143-4185-ae4b-aaf1ebcb42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavloc = os.path.join(os.path.abspath('./data_processed/')\n",
    "\n",
    "if not os.path.exists(wavloc):\n",
    "    print(f\"Directory not found: {wavloc}\")\n",
    "\n",
    "\n",
    "# loop over Audio files\n",
    "for file in os.listdir(wavloc):\n",
    "    print(file)\n",
    "    if 'Mic_nominal_srate16000_denoised' in file:\n",
    "        print('Now processing file '+file)\n",
    "        sessionIndex = file.split('_')[0]   # this is session number\n",
    "        trialIndex = file.split('_')[2] # this is trial number\n",
    "        #load in the audio\n",
    "        print('Loading the audio')\n",
    "        audio_path = os.path.join(wavloc, file)\n",
    "        if not os.path.exists(audio_path):\n",
    "            print(f\"Audio file not found: {audio_path}\")\n",
    "        # input the video with ffmpg\n",
    "        input_audio = ffmpeg.input(audio_path)\n",
    "        print(input_audio)\n",
    "        #load in the video with matchich trialIndex and SessionIndex\n",
    "        print('Loading the video')\n",
    "        video_path = os.path.join(trialfolder, f\"{sessionIndex}_trial_{trialIndex}_video_raw.mp4\")\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video file not found: {video_path}\")\n",
    "        input_video = ffmpeg.input(video_path)\n",
    "        print(input_video)\n",
    "\n",
    "\n",
    "          #combine the audio and video\n",
    "        print('Combining audio and video')\n",
    "        output_path = os.path.abspath(os.path.join(trialfolder, f\"{sessionIndex}_trial_{trialIndex}_final.mp4\"))\n",
    "        ffmpeg.concat(input_video, input_audio, v=1, a=1).output(output_path).run(overwrite_output=True)\n",
    "\n",
    "\n",
    "        #save it\n",
    "        print('Saving the video')\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393caa11-0dd9-4a2d-9467-825bbdbd6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.abspath(videofolder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad653e5-3395-4464-afc7-eb9d773050f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wavloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80369bc7-4efd-4f37-afc2-d9b1da70e1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
